{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **M√≥dulo 3: Escolhendo o modelo certo como escolher um carro**\n",
       "\n",
       "## **Aula 3.1: Modelos base dispon√≠veis**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas como escolher o modelo certo?**\n",
       "\n",
       "Imagine que voc√™ vai comprar um carro. Voc√™ tem v√°rias op√ß√µes:\n",
       "- **Carro pequeno**: Econ√¥mico, f√°cil de estacionar, mas n√£o carrega muita coisa\n",
       "- **SUV**: Espa√ßoso, vers√°til, mas gasta mais combust√≠vel\n",
       "- **Esportivo**: R√°pido, bonito, mas n√£o √© pr√°tico para fam√≠lia\n",
       "\n",
       "**Modelos de IA s√£o a mesma coisa!** Cada um tem suas vantagens e desvantagens.\n",
       "\n",
       "**Por que a escolha do modelo √© importante?**\n",
       "\n",
       "√â como escolher o atleta base para treinar:\n",
       "- **Modelo pequeno**: R√°pido, barato, mas limitado\n",
       "- **Modelo m√©dio**: Equilibrado, bom custo-benef√≠cio\n",
       "- **Modelo grande**: Poderoso, mas lento e caro\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Compara√ß√£o de carros (pequeno, m√©dio, grande) vs modelos de IA\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Importa√ß√µes necess√°rias\n",
       "import torch\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')\n",
       "\n",
       "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **1. Comparando modelos por tamanho**\n",
       "\n",
       "Vamos ver as op√ß√µes dispon√≠veis:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Compara√ß√£o de modelos por tamanho\n",
       "print(\"üöó COMPARA√á√ÉO DE MODELOS POR TAMANHO\\n\")\n",
       "\n",
       "modelos_comparacao = {\n",
       "    \"Pequenos (1-3B)\": {\n",
       "        \"exemplos\": [\"Mistral-7B\", \"Llama-2-7B\", \"T5-small\"],\n",
       "        \"vantagens\": \"R√°pidos, baratos, funcionam em CPU\",\n",
       "        \"desvantagens\": \"Qualidade limitada, menos criativos\",\n",
       "        \"melhor_para\": \"Tarefas simples, prototipagem\",\n",
       "        \"memoria\": \"4-8GB RAM\",\n",
       "        \"tempo_treinamento\": \"2-4 horas\"\n",
       "    },\n",
       "    \"M√©dios (7-13B)\": {\n",
       "        \"exemplos\": [\"Mistral-7B\", \"Llama-2-13B\", \"T5-base\"],\n",
       "        \"vantagens\": \"Bom equil√≠brio qualidade/velocidade\",\n",
       "        \"desvantagens\": \"Precisam de GPU, mais caros\",\n",
       "        \"melhor_para\": \"Aplica√ß√µes em produ√ß√£o\",\n",
       "        \"memoria\": \"8-16GB RAM\",\n",
       "        \"tempo_treinamento\": \"4-8 horas\"\n",
       "    },\n",
       "    \"Grandes (30B+)\": {\n",
       "        \"exemplos\": [\"Llama-2-70B\", \"Mistral-8x7B\", \"T5-large\"],\n",
       "        \"vantagens\": \"Qualidade excepcional, muito criativos\",\n",
       "        \"desvantagens\": \"Muito lentos, caros, precisam de GPU potente\",\n",
       "        \"melhor_para\": \"Pesquisa, aplica√ß√µes cr√≠ticas\",\n",
       "        \"memoria\": \"32GB+ RAM\",\n",
       "        \"tempo_treinamento\": \"12-24 horas\"\n",
       "    }\n",
       "}\n",
       "\n",
       "for tamanho, info in modelos_comparacao.items():\n",
       "    print(f\"üîπ {tamanho}\")\n",
       "    print(f\"   üìù Exemplos: {', '.join(info['exemplos'])}\")\n",
       "    print(f\"   ‚úÖ Vantagens: {info['vantagens']}\")\n",
       "    print(f\"   ‚ö†Ô∏è  Desvantagens: {info['desvantagens']}\")\n",
       "    print(f\"   üéØ Melhor para: {info['melhor_para']}\")\n",
       "    print(f\"   üíæ Mem√≥ria: {info['memoria']}\")\n",
       "    print(f\"   ‚è±Ô∏è  Tempo de treinamento: {info['tempo_treinamento']}\")\n",
       "    print()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **2. Modelos espec√≠ficos para Fine Tuning**\n",
       "\n",
       "Vamos ver os modelos mais populares:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Modelos espec√≠ficos para Fine Tuning\n",
       "print(\"ü§ñ MODELOS ESPEC√çFICOS PARA FINE TUNING\\n\")\n",
       "\n",
       "modelos_especificos = {\n",
       "    \"Mistral-7B-Instruct\": {\n",
       "        \"tamanho\": \"7B par√¢metros\",\n",
       "        \"qualidade\": \"Excelente\",\n",
       "        \"velocidade\": \"R√°pido\",\n",
       "        \"memoria\": \"8GB RAM\",\n",
       "        \"melhor_para\": \"Chat, instru√ß√µes, c√≥digo\",\n",
       "        \"custo\": \"Baixo\",\n",
       "        \"recomendacao\": \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Melhor para come√ßar)\"\n",
       "    },\n",
       "    \"Llama-2-7b-chat\": {\n",
       "        \"tamanho\": \"7B par√¢metros\",\n",
       "        \"qualidade\": \"Muito boa\",\n",
       "        \"velocidade\": \"R√°pido\",\n",
       "        \"memoria\": \"8GB RAM\",\n",
       "        \"melhor_para\": \"Chat, conversa√ß√£o\",\n",
       "        \"custo\": \"Baixo\",\n",
       "        \"recomendacao\": \"‚≠ê‚≠ê‚≠ê‚≠ê (Muito bom)\"\n",
       "    },\n",
       "    \"T5-base\": {\n",
       "        \"tamanho\": \"220M par√¢metros\",\n",
       "        \"qualidade\": \"Boa\",\n",
       "        \"velocidade\": \"Muito r√°pido\",\n",
       "        \"memoria\": \"2GB RAM\",\n",
       "        \"melhor_para\": \"Tarefas espec√≠ficas, classifica√ß√£o\",\n",
       "        \"custo\": \"Muito baixo\",\n",
       "        \"recomendacao\": \"‚≠ê‚≠ê‚≠ê (Bom para tarefas simples)\"\n",
       "    },\n",
       "    \"DistilBERT\": {\n",
       "        \"tamanho\": \"66M par√¢metros\",\n",
       "        \"qualidade\": \"Boa\",\n",
       "        \"velocidade\": \"Muito r√°pido\",\n",
       "        \"memoria\": \"1GB RAM\",\n",
       "        \"melhor_para\": \"Classifica√ß√£o, an√°lise de sentimentos\",\n",
       "        \"custo\": \"Muito baixo\",\n",
       "        \"recomendacao\": \"‚≠ê‚≠ê‚≠ê (Perfeito para classifica√ß√£o)\"\n",
       "    }\n",
       "}\n",
       "\n",
       "for modelo, info in modelos_especificos.items():\n",
       "    print(f\"üîπ {modelo}\")\n",
       "    print(f\"   üìè Tamanho: {info['tamanho']}\")\n",
       "    print(f\"   üéØ Qualidade: {info['qualidade']}\")\n",
       "    print(f\"   ‚ö° Velocidade: {info['velocidade']}\")\n",
       "    print(f\"   üíæ Mem√≥ria: {info['memoria']}\")\n",
       "    print(f\"   üéØ Melhor para: {info['melhor_para']}\")\n",
       "    print(f\"   üí∞ Custo: {info['custo']}\")\n",
       "    print(f\"   ‚≠ê Recomenda√ß√£o: {info['recomendacao']}\")\n",
       "    print()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **3. LoRA e QLoRA - Fine Tuning eficiente**\n",
       "\n",
       "Agora vamos falar das t√©cnicas que revolucionaram o Fine Tuning:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Explicando LoRA e QLoRA\n",
       "print(\"üöÄ LORA E QLORA - FINE TUNING EFICIENTE\\n\")\n",
       "\n",
       "print(\"üîπ O que √© LoRA?\")\n",
       "print(\"   LoRA = Low-Rank Adaptation\")\n",
       "print(\"   √â como adicionar 'extens√µes' ao modelo em vez de modificar tudo\")\n",
       "print(\"   Analogia: √â como instalar um 'plugin' no Photoshop\")\n",
       "print()\n",
       "\n",
       "print(\"üîπ O que √© QLoRA?\")\n",
       "print(\"   QLoRA = Quantized LoRA\")\n",
       "print(\"   LoRA + quantiza√ß√£o para usar menos mem√≥ria\")\n",
       "print(\"   Analogia: √â como comprimir um arquivo ZIP\")\n",
       "print()\n",
       "\n",
       "# Compara√ß√£o de t√©cnicas\n",
       "tecnicas_comparacao = {\n",
       "    \"Fine Tuning Completo\": {\n",
       "        \"memoria\": \"Muito alta (32GB+)\",\n",
       "        \"tempo\": \"Muito lento (24h+)\",\n",
       "        \"custo\": \"Muito alto\",\n",
       "        \"qualidade\": \"Excelente\",\n",
       "        \"quando_usar\": \"Raramente, apenas para casos cr√≠ticos\"\n",
       "    },\n",
       "    \"LoRA\": {\n",
       "        \"memoria\": \"M√©dia (8-16GB)\",\n",
       "        \"tempo\": \"R√°pido (2-4h)\",\n",
       "        \"custo\": \"Baixo\",\n",
       "        \"qualidade\": \"Muito boa\",\n",
       "        \"quando_usar\": \"Maioria dos casos\"\n",
       "    },\n",
       "    \"QLoRA\": {\n",
       "        \"memoria\": \"Baixa (4-8GB)\",\n",
       "        \"tempo\": \"Muito r√°pido (1-2h)\",\n",
       "        \"custo\": \"Muito baixo\",\n",
       "        \"qualidade\": \"Boa\",\n",
       "        \"quando_usar\": \"Recursos limitados, prototipagem\"\n",
       "    }\n",
       "}\n",
       "\n",
       "print(\"üìä COMPARA√á√ÉO DAS T√âCNICAS:\\n\")\n",
       "\n",
       "for tecnica, info in tecnicas_comparacao.items():\n",
       "    print(f\"üîπ {tecnica}\")\n",
       "    print(f\"   üíæ Mem√≥ria: {info['memoria']}\")\n",
       "    print(f\"   ‚è±Ô∏è  Tempo: {info['tempo']}\")\n",
       "    print(f\"   üí∞ Custo: {info['custo']}\")\n",
       "    print(f\"   üéØ Qualidade: {info['qualidade']}\")\n",
       "    print(f\"   üéØ Quando usar: {info['quando_usar']}\")\n",
       "    print()\n",
       "\n",
       "print(\"üí° Para 99% dos casos, LoRA ou QLoRA s√£o suficientes!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **4. Exemplo Pr√°tico: Carregando diferentes modelos**\n",
       "\n",
       "Vamos ver como carregar modelos diferentes:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Fun√ß√£o para testar carregamento de modelos\n",
       "def testar_carregamento_modelo(model_name, model_type=\"causal\"):\n",
       "    \"\"\"\n",
       "    Testa o carregamento de um modelo\n",
       "    \"\"\"\n",
       "    try:\n",
       "        print(f\"üîç Testando: {model_name}\")\n",
       "        \n",
       "        # Carregando tokenizer\n",
       "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "        \n",
       "        # Carregando modelo\n",
       "        if model_type == \"causal\":\n",
       "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "        elif model_type == \"classification\":\n",
       "            model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
       "        \n",
       "        # Calculando par√¢metros\n",
       "        num_params = model.num_parameters()\n",
       "        \n",
       "        # Estimando mem√≥ria (aproximado)\n",
       "        memoria_estimada = num_params * 4 / (1024**3)  # 4 bytes por par√¢metro\n",
       "        \n",
       "        print(f\"   ‚úÖ Carregado com sucesso!\")\n",
       "        print(f\"   üìä Par√¢metros: {num_params:,}\")\n",
       "        print(f\"   üíæ Mem√≥ria estimada: {memoria_estimada:.1f} GB\")\n",
       "        \n",
       "        return True, num_params, memoria_estimada\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"   ‚ùå Erro: {e}\")\n",
       "        return False, 0, 0\n",
       "\n",
       "# Testando modelos pequenos (mais seguros para demo)\n",
       "print(\"üß™ TESTANDO CARREGAMENTO DE MODELOS\\n\")\n",
       "\n",
       "modelos_teste = [\n",
       "    {\"nome\": \"microsoft/DialoGPT-small\", \"tipo\": \"causal\"},\n",
       "    {\"nome\": \"distilbert-base-uncased\", \"tipo\": \"classification\"},\n",
       "    {\"nome\": \"t5-small\", \"tipo\": \"causal\"}\n",
       "]\n",
       "\n",
       "resultados = []\n",
       "\n",
       "for modelo_info in modelos_teste:\n",
       "    sucesso, params, memoria = testar_carregamento_modelo(\n",
       "        modelo_info[\"nome\"], \n",
       "        modelo_info[\"tipo\"]\n",
       "    )\n",
       "    \n",
       "    if sucesso:\n",
       "        resultados.append({\n",
       "            \"nome\": modelo_info[\"nome\"],\n",
       "            \"parametros\": params,\n",
       "            \"memoria\": memoria\n",
       "        })\n",
       "    \n",
       "    print()\n",
       "\n",
       "print(\"üí° Estes s√£o modelos pequenos para demonstra√ß√£o. Para produ√ß√£o, use modelos maiores!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **5. Visualizando compara√ß√£o de modelos**\n",
       "\n",
       "Vamos criar gr√°ficos para comparar os modelos:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando visualiza√ß√µes comparativas\n",
       "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
       "\n",
       "# Dados para visualiza√ß√£o\n",
       "modelos_viz = {\n",
       "    \"T5-small\": {\"parametros\": 60e6, \"memoria\": 0.2, \"qualidade\": 6, \"velocidade\": 9},\n",
       "    \"DistilBERT\": {\"parametros\": 66e6, \"memoria\": 0.3, \"qualidade\": 7, \"velocidade\": 9},\n",
       "    \"Mistral-7B\": {\"parametros\": 7e9, \"memoria\": 8, \"qualidade\": 9, \"velocidade\": 7},\n",
       "    \"Llama-2-7B\": {\"parametros\": 7e9, \"memoria\": 8, \"qualidade\": 8, \"velocidade\": 7},\n",
       "    \"Llama-2-13B\": {\"parametros\": 13e9, \"memoria\": 16, \"qualidade\": 9, \"velocidade\": 5},\n",
       "    \"Llama-2-70B\": {\"parametros\": 70e9, \"memoria\": 32, \"qualidade\": 10, \"velocidade\": 2}\n",
       "}\n",
       "\n",
       "nomes = list(modelos_viz.keys())\n",
       "parametros = [modelos_viz[nome][\"parametros\"] for nome in nomes]\n",
       "memoria = [modelos_viz[nome][\"memoria\"] for nome in nomes]\n",
       "qualidade = [modelos_viz[nome][\"qualidade\"] for nome in nomes]\n",
       "velocidade = [modelos_viz[nome][\"velocidade\"] for nome in nomes]\n",
       "\n",
       "# 1. Par√¢metros vs Mem√≥ria\n",
       "ax1.scatter(parametros, memoria, s=100, alpha=0.7)\n",
       "for i, nome in enumerate(nomes):\n",
       "    ax1.annotate(nome, (parametros[i], memoria[i]), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
       "ax1.set_xlabel('Par√¢metros')\n",
       "ax1.set_ylabel('Mem√≥ria (GB)')\n",
       "ax1.set_title('üìä Par√¢metros vs Mem√≥ria')\n",
       "ax1.set_xscale('log')\n",
       "\n",
       "# 2. Qualidade vs Velocidade\n",
       "ax2.scatter(qualidade, velocidade, s=100, alpha=0.7, c='orange')\n",
       "for i, nome in enumerate(nomes):\n",
       "    ax2.annotate(nome, (qualidade[i], velocidade[i]), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
       "ax2.set_xlabel('Qualidade (1-10)')\n",
       "ax2.set_ylabel('Velocidade (1-10)')\n",
       "ax2.set_title('‚ö° Qualidade vs Velocidade')\n",
       "\n",
       "# 3. Distribui√ß√£o de tamanhos\n",
       "tamanhos = ['Pequeno', 'M√©dio', 'Grande']\n",
       "contagem = [2, 2, 2]  # Baseado nos modelos acima\n",
       "ax3.bar(tamanhos, contagem, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
       "ax3.set_title('üìè Distribui√ß√£o por Tamanho')\n",
       "ax3.set_ylabel('N√∫mero de Modelos')\n",
       "\n",
       "# 4. Radar chart para Mistral-7B\n",
       "categorias = ['Qualidade', 'Velocidade', 'Custo-Benef√≠cio', 'Facilidade', 'Mem√≥ria']\n",
       "valores = [9, 7, 9, 8, 6]  # Valores para Mistral-7B\n",
       "\n",
       "angles = np.linspace(0, 2 * np.pi, len(categorias), endpoint=False).tolist()\n",
       "valores += valores[:1]  # Fechar o radar\n",
       "angles += angles[:1]\n",
       "\n",
       "ax4.plot(angles, valores, 'o-', linewidth=2, label='Mistral-7B')\n",
       "ax4.fill(angles, valores, alpha=0.25)\n",
       "ax4.set_xticks(angles[:-1])\n",
       "ax4.set_xticklabels(categorias)\n",
       "ax4.set_ylim(0, 10)\n",
       "ax4.set_title('üéØ Perfil do Mistral-7B')\n",
       "ax4.legend()\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "print(\"üí° Mistral-7B √© geralmente a melhor escolha para come√ßar: bom equil√≠brio entre qualidade e recursos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **6. Guia de escolha de modelo**\n",
       "\n",
       "Vamos criar um guia pr√°tico:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Guia de escolha de modelo\n",
       "print(\"üéØ GUIA DE ESCOLHA DE MODELO\\n\")\n",
       "\n",
       "def recomendar_modelo(recursos, tarefa, urgencia):\n",
       "    \"\"\"\n",
       "    Recomenda um modelo baseado nos crit√©rios\n",
       "    \"\"\"\n",
       "    print(f\"üîç Analisando seus crit√©rios:\")\n",
       "    print(f\"   üíæ Recursos: {recursos}\")\n",
       "    print(f\"   üéØ Tarefa: {tarefa}\")\n",
       "    print(f\"   ‚è±Ô∏è  Urg√™ncia: {urgencia}\")\n",
       "    print()\n",
       "    \n",
       "    if recursos == \"limitados\" and urgencia == \"alta\":\n",
       "        print(\"üöÄ RECOMENDA√á√ÉO: T5-small ou DistilBERT\")\n",
       "        print(\"   ‚úÖ R√°pido, barato, funciona em CPU\")\n",
       "        print(\"   ‚ö†Ô∏è  Qualidade limitada\")\n",
       "    elif recursos == \"limitados\" and urgencia == \"m√©dia\":\n",
       "        print(\"üöÄ RECOMENDA√á√ÉO: Mistral-7B com QLoRA\")\n",
       "        print(\"   ‚úÖ Boa qualidade, baixo custo\")\n",
       "        print(\"   ‚ö†Ô∏è  Precisa de GPU b√°sica\")\n",
       "    elif recursos == \"adequados\" and urgencia == \"m√©dia\":\n",
       "        print(\"üöÄ RECOMENDA√á√ÉO: Mistral-7B com LoRA\")\n",
       "        print(\"   ‚úÖ Excelente qualidade, custo moderado\")\n",
       "        print(\"   ‚ö†Ô∏è  Precisa de GPU\")\n",
       "    elif recursos == \"abundantes\" and urgencia == \"baixa\":\n",
       "        print(\"üöÄ RECOMENDA√á√ÉO: Llama-2-13B com LoRA\")\n",
       "        print(\"   ‚úÖ Qualidade excepcional\")\n",
       "        print(\"   ‚ö†Ô∏è  Alto custo e tempo\")\n",
       "    else:\n",
       "        print(\"üöÄ RECOMENDA√á√ÉO: Mistral-7B com LoRA\")\n",
       "        print(\"   ‚úÖ Melhor custo-benef√≠cio geral\")\n",
       "\n",
       "# Exemplos de uso\n",
       "print(\"üìã EXEMPLOS DE RECOMENDA√á√ïES:\\n\")\n",
       "\n",
       "exemplos = [\n",
       "    {\"recursos\": \"limitados\", \"tarefa\": \"classifica√ß√£o\", \"urgencia\": \"alta\"},\n",
       "    {\"recursos\": \"adequados\", \"tarefa\": \"chat\", \"urgencia\": \"m√©dia\"},\n",
       "    {\"recursos\": \"abundantes\", \"tarefa\": \"an√°lise complexa\", \"urgencia\": \"baixa\"}\n",
       "]\n",
       "\n",
       "for i, exemplo in enumerate(exemplos, 1):\n",
       "    print(f\"üîπ Exemplo {i}:\")\n",
       "    recomendar_modelo(\n",
       "        exemplo[\"recursos\"], \n",
       "        exemplo[\"tarefa\"], \n",
       "        exemplo[\"urgencia\"]\n",
       "    )\n",
       "    print()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **7. Teste R√°pido**\n",
       "\n",
       "Vamos testar seu entendimento sobre escolha de modelos:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Teste r√°pido sobre escolha de modelos\n",
       "print(\"üß™ TESTE R√ÅPIDO - ESCOLHA DE MODELOS\\n\")\n",
       "\n",
       "perguntas_teste = [\n",
       "    {\n",
       "        \"pergunta\": \"Qual t√©cnica √© mais eficiente em termos de mem√≥ria?\",\n",
       "        \"opcoes\": [\"A) Fine Tuning completo\", \"B) LoRA\", \"C) QLoRA\"],\n",
       "        \"resposta\": \"C\",\n",
       "        \"explicacao\": \"QLoRA combina LoRA com quantiza√ß√£o, usando menos mem√≥ria!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"Qual modelo √© melhor para come√ßar com Fine Tuning?\",\n",
       "        \"opcoes\": [\"A) Llama-2-70B\", \"B) Mistral-7B\", \"C) T5-small\"],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"Mistral-7B oferece o melhor equil√≠brio qualidade/recursos!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"Para classifica√ß√£o de sentimentos, qual modelo √© mais adequado?\",\n",
       "        \"opcoes\": [\"A) Mistral-7B\", \"B) DistilBERT\", \"C) Llama-2-70B\"],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"DistilBERT √© otimizado para tarefas de classifica√ß√£o!\"\n",
       "    }\n",
       "]\n",
       "\n",
       "for i, q in enumerate(perguntas_teste, 1):\n",
       "    print(f\"‚ùì {i}. {q['pergunta']}\")\n",
       "    for opcao in q['opcoes']:\n",
       "        print(f\"   {opcao}\")\n",
       "    print(f\"üí° Resposta: {q['resposta']} - {q['explicacao']}\")\n",
       "    print()\n",
       "\n",
       "print(\"üéâ Parab√©ns! Voc√™ j√° entende como escolher modelos para Fine Tuning!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **8. Desafio do M√≥dulo**\n",
       "\n",
       "Agora √© sua vez de escolher:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Desafio do m√≥dulo\n",
       "print(\"üéØ DESAFIO DO M√ìDULO\\n\")\n",
       "\n",
       "print(\"Escolha o modelo ideal para cada cen√°rio:\")\n",
       "print(\"\\n1Ô∏è‚É£ Startup com recursos limitados quer criar um chatbot de atendimento\")\n",
       "print(\"   - Or√ßamento: Baixo\")\n",
       "print(\"   - Infraestrutura: 1 GPU b√°sica\")\n",
       "print(\"   - Urg√™ncia: Alta (lan√ßamento em 2 semanas)\")\n",
       "\n",
       "print(\"\\n2Ô∏è‚É£ Empresa m√©dia quer melhorar an√°lise de sentimentos de clientes\")\n",
       "print(\"   - Or√ßamento: M√©dio\")\n",
       "print(\"   - Infraestrutura: 2 GPUs\")\n",
       "print(\"   - Urg√™ncia: M√©dia (3 meses)\")\n",
       "\n",
       "print(\"\\n3Ô∏è‚É£ Grande empresa quer criar assistente especializado em produtos\")\n",
       "print(\"   - Or√ßamento: Alto\")\n",
       "print(\"   - Infraestrutura: Cluster de GPUs\")\n",
       "print(\"   - Urg√™ncia: Baixa (6 meses)\")\n",
       "\n",
       "print(\"\\nüìù Para cada cen√°rio, escolha:\")\n",
       "print(\"   - Modelo base\")\n",
       "print(\"   - T√©cnica de fine tuning\")\n",
       "print(\"   - Justificativa da escolha\")\n",
       "\n",
       "print(\"\\nüöÄ No pr√≥ximo m√≥dulo, vamos aprender a configurar e treinar os modelos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üéâ M√≥dulo 3 Conclu√≠do!**\n",
       "\n",
       "### **O que aprendemos:**\n",
       "\n",
       "‚úÖ **Compara√ß√£o de modelos por tamanho** (Pequenos, M√©dios, Grandes)  \n",
       "‚úÖ **Modelos espec√≠ficos para Fine Tuning** (Mistral, Llama, T5, DistilBERT)  \n",
       "‚úÖ **LoRA e QLoRA** (t√©cnicas eficientes)  \n",
       "‚úÖ **Como carregar e testar modelos**  \n",
       "‚úÖ **Visualiza√ß√£o comparativa**  \n",
       "‚úÖ **Guia de escolha de modelo**\n",
       "\n",
       "### **Pr√≥ximos Passos:**\n",
       "\n",
       "üöÄ **M√≥dulo 4**: Treinando como um personal trainer  \n",
       "üöÄ **M√≥dulo 5**: Avaliando como um professor corrige prova  \n",
       "üöÄ **M√≥dulo 6**: Deploy como abrir um restaurante\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica do Instrutor**: Escolher o modelo certo √© como escolher o carro certo - depende do seu or√ßamento, necessidades e urg√™ncia! Agora que escolhemos o \"carro\", vamos aprender a \"dirigir\" (treinar)! üòÑ\n",
       "\n",
       "**üöÄ Pr√≥ximo m√≥dulo**: Vamos configurar e treinar nossos modelos!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "personal trainer",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }