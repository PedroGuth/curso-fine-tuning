{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **M√≥dulo 5: Avaliando como um professor corrige prova**\n",
       "\n",
       "## **Aula 5.1: M√©tricas de avalia√ß√£o**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas como saber se a IA aprendeu bem?**\n",
       "\n",
       "Imagine que voc√™ √© um professor e acabou de corrigir as provas dos alunos. Voc√™ n√£o vai s√≥ dar uma nota, vai analisar:\n",
       "- ‚úÖ **Acertos vs Erros**: Quantas quest√µes acertou?\n",
       "- ‚úÖ **Qualidade das respostas**: Est√° explicando bem?\n",
       "- ‚úÖ **Consist√™ncia**: Est√° sempre certo ou √†s vezes erra?\n",
       "- ‚úÖ **Compara√ß√£o**: Est√° melhor que antes?\n",
       "\n",
       "**Avaliar modelos de IA √© a mesma coisa!** Vamos ver se nossa IA \"estudou\" bem.\n",
       "\n",
       "**Por que a avalia√ß√£o √© importante?**\n",
       "\n",
       "√â como verificar se o treino funcionou:\n",
       "- **Avalia√ß√£o ruim**: IA que n√£o aprendeu nada\n",
       "- **Avalia√ß√£o boa**: IA que virou especialista\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Professor corrigindo prova vs m√©tricas de avalia√ß√£o de IA\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Importa√ß√µes necess√°rias\n",
       "import torch\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
       "import json\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')\n",
       "\n",
       "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **1. Carregando modelo e dados de teste**\n",
       "\n",
       "Vamos carregar nosso modelo treinado e dados para avalia√ß√£o:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Carregando modelo treinado (simulado)\n",
       "print(\"ü§ñ CARREGANDO MODELO PARA AVALIA√á√ÉO\\n\")\n",
       "\n",
       "# Para demonstra√ß√£o, vamos usar o modelo base\n",
       "model_name = \"microsoft/DialoGPT-small\"\n",
       "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "\n",
       "print(f\"‚úÖ Modelo carregado: {model_name}\")\n",
       "print(f\"üìä Par√¢metros: {model.num_parameters():,}\")\n",
       "\n",
       "# Carregando dados de teste\n",
       "print(\"\\nüìä CARREGANDO DADOS DE TESTE\\n\")\n",
       "\n",
       "try:\n",
       "    with open('datasets/exemplo_vendas.json', 'r', encoding='utf-8') as f:\n",
       "        dados_teste = json.load(f)\n",
       "    \n",
       "    print(f\"‚úÖ Dados de teste carregados: {len(dados_teste)} exemplos\")\n",
       "    \n",
       "except FileNotFoundError:\n",
       "    print(\"‚ö†Ô∏è  Dados de teste n√£o encontrados. Criando exemplos...\")\n",
       "    dados_teste = [\n",
       "        {\n",
       "            \"messages\": [\n",
       "                {\"role\": \"user\", \"content\": \"Qual √© o melhor iPhone?\"},\n",
       "                {\"role\": \"assistant\", \"content\": \"O iPhone 15 Pro Max √© o melhor para fotografia e performance.\"}\n",
       "            ]\n",
       "        },\n",
       "        {\n",
       "            \"messages\": [\n",
       "                {\"role\": \"user\", \"content\": \"Quanto custa o iPhone 15?\"},\n",
       "                {\"role\": \"assistant\", \"content\": \"O iPhone 15 custa R$ 6.999.\"}\n",
       "            ]\n",
       "        }\n",
       "    ]\n",
       "\n",
       "print(\"‚úÖ Dados preparados para avalia√ß√£o!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **2. Fun√ß√£o de gera√ß√£o de respostas**\n",
       "\n",
       "Vamos criar uma fun√ß√£o para gerar respostas do modelo:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Fun√ß√£o para gerar respostas\n",
       "def gerar_resposta(model, tokenizer, pergunta, max_length=100):\n",
       "    \"\"\"\n",
       "    Gera resposta do modelo para uma pergunta\n",
       "    \"\"\"\n",
       "    try:\n",
       "        # Formatando entrada\n",
       "        input_text = f\"User: {pergunta}\\nAssistant:\"\n",
       "        \n",
       "        # Tokenizando\n",
       "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
       "        \n",
       "        # Gerando resposta\n",
       "        with torch.no_grad():\n",
       "            outputs = model.generate(\n",
       "                inputs[\"input_ids\"],\n",
       "                max_length=max_length,\n",
       "                num_return_sequences=1,\n",
       "                temperature=0.7,\n",
       "                do_sample=True,\n",
       "                pad_token_id=tokenizer.eos_token_id\n",
       "            )\n",
       "        \n",
       "        # Decodificando resposta\n",
       "        resposta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "        \n",
       "        # Extraindo apenas a parte do assistant\n",
       "        if \"Assistant:\" in resposta_completa:\n",
       "            resposta = resposta_completa.split(\"Assistant:\")[-1].strip()\n",
       "        else:\n",
       "            resposta = resposta_completa\n",
       "        \n",
       "        return resposta\n",
       "        \n",
       "    except Exception as e:\n",
       "        return f\"Erro na gera√ß√£o: {e}\"\n",
       "\n",
       "# Testando gera√ß√£o\n",
       "print(\"üß™ TESTANDO GERA√á√ÉO DE RESPOSTAS\\n\")\n",
       "\n",
       "pergunta_teste = \"Qual √© o melhor iPhone para fotografia?\"\n",
       "resposta_gerada = gerar_resposta(model, tokenizer, pergunta_teste)\n",
       "\n",
       "print(f\"‚ùì Pergunta: {pergunta_teste}\")\n",
       "print(f\"ü§ñ Resposta gerada: {resposta_gerada}\")\n",
       "\n",
       "print(\"\\n‚úÖ Fun√ß√£o de gera√ß√£o funcionando!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **3. M√©tricas de avalia√ß√£o autom√°tica**\n",
       "\n",
       "Vamos implementar m√©tricas para avaliar o modelo:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Fun√ß√µes de m√©tricas de avalia√ß√£o\n",
       "def calcular_bleu_score(resposta_gerada, resposta_esperada):\n",
       "    \"\"\"\n",
       "    Calcula BLEU score (simplificado)\n",
       "    \"\"\"\n",
       "    from nltk.translate.bleu_score import sentence_bleu\n",
       "    \n",
       "    try:\n",
       "        # Tokenizando\n",
       "        referencia = resposta_esperada.lower().split()\n",
       "        candidato = resposta_gerada.lower().split()\n",
       "        \n",
       "        # Calculando BLEU\n",
       "        score = sentence_bleu([referencia], candidato)\n",
       "        return score\n",
       "    except:\n",
       "        return 0.0\n",
       "\n",
       "def calcular_similaridade_cosseno(texto1, texto2):\n",
       "    \"\"\"\n",
       "    Calcula similaridade de cosseno entre dois textos\n",
       "    \"\"\"\n",
       "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "    from sklearn.metrics.pairwise import cosine_similarity\n",
       "    \n",
       "    try:\n",
       "        vectorizer = TfidfVectorizer()\n",
       "        tfidf_matrix = vectorizer.fit_transform([texto1, texto2])\n",
       "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
       "        return similarity\n",
       "    except:\n",
       "        return 0.0\n",
       "\n",
       "def avaliar_resposta(resposta_gerada, resposta_esperada):\n",
       "    \"\"\"\n",
       "    Avalia uma resposta gerada\n",
       "    \"\"\"\n",
       "    # BLEU Score\n",
       "    bleu = calcular_bleu_score(resposta_gerada, resposta_esperada)\n",
       "    \n",
       "    # Similaridade de cosseno\n",
       "    similaridade = calcular_similaridade_cosseno(resposta_gerada, resposta_esperada)\n",
       "    \n",
       "    # Comprimento da resposta\n",
       "    comprimento = len(resposta_gerada.split())\n",
       "    \n",
       "    # Score composto\n",
       "    score_final = (bleu * 0.4 + similaridade * 0.4 + min(comprimento/20, 1.0) * 0.2)\n",
       "    \n",
       "    return {\n",
       "        \"bleu\": bleu,\n",
       "        \"similaridade\": similaridade,\n",
       "        \"comprimento\": comprimento,\n",
       "        \"score_final\": score_final\n",
       "    }\n",
       "\n",
       "print(\"üìä FUN√á√ïES DE AVALIA√á√ÉO CRIADAS\\n\")\n",
       "\n",
       "# Testando avalia√ß√£o\n",
       "resposta_esperada = \"O iPhone 15 Pro Max √© o melhor para fotografia, com c√¢mera de 48MP e zoom √≥ptico de 5x.\"\n",
       "avaliacao = avaliar_resposta(resposta_gerada, resposta_esperada)\n",
       "\n",
       "print(\"üìù Exemplo de avalia√ß√£o:\")\n",
       "print(f\"   BLEU Score: {avaliacao['bleu']:.3f}\")\n",
       "print(f\"   Similaridade: {avaliacao['similaridade']:.3f}\")\n",
       "print(f\"   Comprimento: {avaliacao['comprimento']} palavras\")\n",
       "print(f\"   Score Final: {avaliacao['score_final']:.3f}\")\n",
       "\n",
       "print(\"\\n‚úÖ Fun√ß√µes de avalia√ß√£o funcionando!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **4. Avalia√ß√£o completa do modelo**\n",
       "\n",
       "Vamos avaliar o modelo em todo o dataset de teste:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Avalia√ß√£o completa\n",
       "print(\"üîç AVALIA√á√ÉO COMPLETA DO MODELO\\n\")\n",
       "\n",
       "resultados_avaliacao = []\n",
       "\n",
       "for i, exemplo in enumerate(dados_teste):\n",
       "    pergunta = exemplo['messages'][0]['content']\n",
       "    resposta_esperada = exemplo['messages'][1]['content']\n",
       "    \n",
       "    # Gerando resposta\n",
       "    resposta_gerada = gerar_resposta(model, tokenizer, pergunta)\n",
       "    \n",
       "    # Avaliando\n",
       "    avaliacao = avaliar_resposta(resposta_gerada, resposta_esperada)\n",
       "    \n",
       "    resultados_avaliacao.append({\n",
       "        \"pergunta\": pergunta,\n",
       "        \"resposta_esperada\": resposta_esperada,\n",
       "        \"resposta_gerada\": resposta_gerada,\n",
       "        \"bleu\": avaliacao['bleu'],\n",
       "        \"similaridade\": avaliacao['similaridade'],\n",
       "        \"comprimento\": avaliacao['comprimento'],\n",
       "        \"score_final\": avaliacao['score_final']\n",
       "    })\n",
       "    \n",
       "    print(f\"üìù Exemplo {i+1}:\")\n",
       "    print(f\"   ‚ùì {pergunta}\")\n",
       "    print(f\"   ‚úÖ Esperada: {resposta_esperada[:50]}...\")\n",
       "    print(f\"   ü§ñ Gerada: {resposta_gerada[:50]}...\")\n",
       "    print(f\"   üìä Score: {avaliacao['score_final']:.3f}\")\n",
       "    print()\n",
       "\n",
       "print(f\"‚úÖ Avalia√ß√£o conclu√≠da para {len(resultados_avaliacao)} exemplos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **5. An√°lise estat√≠stica dos resultados**\n",
       "\n",
       "Vamos analisar os resultados estatisticamente:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# An√°lise estat√≠stica\n",
       "print(\"üìà AN√ÅLISE ESTAT√çSTICA DOS RESULTADOS\\n\")\n",
       "\n",
       "# Convertendo para DataFrame\n",
       "df_resultados = pd.DataFrame(resultados_avaliacao)\n",
       "\n",
       "# Estat√≠sticas gerais\n",
       "print(\"üìä ESTAT√çSTICAS GERAIS:\")\n",
       "print(f\"   N√∫mero de exemplos: {len(df_resultados)}\")\n",
       "print(f\"   BLEU Score m√©dio: {df_resultados['bleu'].mean():.3f}\")\n",
       "print(f\"   Similaridade m√©dia: {df_resultados['similaridade'].mean():.3f}\")\n",
       "print(f\"   Score final m√©dio: {df_resultados['score_final'].mean():.3f}\")\n",
       "print(f\"   Comprimento m√©dio: {df_resultados['comprimento'].mean():.1f} palavras\")\n",
       "\n",
       "# Melhores e piores resultados\n",
       "melhor_idx = df_resultados['score_final'].idxmax()\n",
       "pior_idx = df_resultados['score_final'].idxmin()\n",
       "\n",
       "print(f\"\\nüèÜ MELHOR RESULTADO (Score: {df_resultados.loc[melhor_idx, 'score_final']:.3f}):\")\n",
       "print(f\"   Pergunta: {df_resultados.loc[melhor_idx, 'pergunta']}\")\n",
       "print(f\"   Resposta gerada: {df_resultados.loc[melhor_idx, 'resposta_gerada']}\")\n",
       "\n",
       "print(f\"\\n‚ö†Ô∏è  PIOR RESULTADO (Score: {df_resultados.loc[pior_idx, 'score_final']:.3f}):\")\n",
       "print(f\"   Pergunta: {df_resultados.loc[pior_idx, 'pergunta']}\")\n",
       "print(f\"   Resposta gerada: {df_resultados.loc[pior_idx, 'resposta_gerada']}\")\n",
       "\n",
       "print(\"\\n‚úÖ An√°lise estat√≠stica conclu√≠da!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **6. Visualiza√ß√£o dos resultados**\n",
       "\n",
       "Vamos criar gr√°ficos para visualizar os resultados:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando visualiza√ß√µes\n",
       "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
       "\n",
       "# 1. Distribui√ß√£o dos scores finais\n",
       "ax1.hist(df_resultados['score_final'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
       "ax1.set_title('üìä Distribui√ß√£o dos Scores Finais')\n",
       "ax1.set_xlabel('Score Final')\n",
       "ax1.set_ylabel('Frequ√™ncia')\n",
       "ax1.axvline(df_resultados['score_final'].mean(), color='red', linestyle='--', \n",
       "            label=f'M√©dia: {df_resultados[\"score_final\"].mean():.3f}')\n",
       "ax1.legend()\n",
       "\n",
       "# 2. BLEU vs Similaridade\n",
       "ax2.scatter(df_resultados['bleu'], df_resultados['similaridade'], alpha=0.7, color='lightgreen')\n",
       "ax2.set_title('üîç BLEU Score vs Similaridade')\n",
       "ax2.set_xlabel('BLEU Score')\n",
       "ax2.set_ylabel('Similaridade de Cosseno')\n",
       "\n",
       "# 3. Comprimento vs Score\n",
       "ax3.scatter(df_resultados['comprimento'], df_resultados['score_final'], alpha=0.7, color='orange')\n",
       "ax3.set_title('üìè Comprimento vs Score Final')\n",
       "ax3.set_xlabel('Comprimento (palavras)')\n",
       "ax3.set_ylabel('Score Final')\n",
       "\n",
       "# 4. Compara√ß√£o de m√©tricas\n",
       "metricas = ['BLEU', 'Similaridade', 'Score Final']\n",
       "valores_medios = [\n",
       "    df_resultados['bleu'].mean(),\n",
       "    df_resultados['similaridade'].mean(),\n",
       "    df_resultados['score_final'].mean()\n",
       "]\n",
       "\n",
       "bars = ax4.bar(metricas, valores_medios, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
       "ax4.set_title('üìà Compara√ß√£o de M√©tricas')\n",
       "ax4.set_ylabel('Valor M√©dio')\n",
       "ax4.set_ylim(0, 1)\n",
       "\n",
       "# Adicionando valores nas barras\n",
       "for bar, valor in zip(bars, valores_medios):\n",
       "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
       "             f'{valor:.3f}', ha='center', va='bottom')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "print(\"üí° Estes gr√°ficos ajudam a entender o desempenho do modelo em diferentes aspectos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **7. Teste A/B com modelo base**\n",
       "\n",
       "Vamos comparar com o modelo base (antes do fine tuning):"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Teste A/B (simulado)\n",
       "print(\"üîÑ TESTE A/B: MODELO BASE vs FINE TUNED\\n\")\n",
       "\n",
       "# Simulando resultados do modelo base\n",
       "resultados_base = {\n",
       "    \"bleu_medio\": 0.15,  # Menor que o fine tuned\n",
       "    \"similaridade_media\": 0.25,  # Menor que o fine tuned\n",
       "    \"score_final_medio\": 0.20,  # Menor que o fine tuned\n",
       "    \"comprimento_medio\": 8.5  # Menor que o fine tuned\n",
       "}\n",
       "\n",
       "# Resultados do modelo fine tuned\n",
       "resultados_finetuned = {\n",
       "    \"bleu_medio\": df_resultados['bleu'].mean(),\n",
       "    \"similaridade_media\": df_resultados['similaridade'].mean(),\n",
       "    \"score_final_medio\": df_resultados['score_final'].mean(),\n",
       "    \"comprimento_medio\": df_resultados['comprimento'].mean()\n",
       "}\n",
       "\n",
       "print(\"üìä COMPARA√á√ÉO A/B:\")\n",
       "print(f\"\\nüîπ Modelo Base:\")\n",
       "print(f\"   BLEU: {resultados_base['bleu_medio']:.3f}\")\n",
       "print(f\"   Similaridade: {resultados_base['similaridade_media']:.3f}\")\n",
       "print(f\"   Score Final: {resultados_base['score_final_medio']:.3f}\")\n",
       "print(f\"   Comprimento: {resultados_base['comprimento_medio']:.1f} palavras\")\n",
       "\n",
       "print(f\"\\nüîπ Modelo Fine Tuned:\")\n",
       "print(f\"   BLEU: {resultados_finetuned['bleu_medio']:.3f}\")\n",
       "print(f\"   Similaridade: {resultados_finetuned['similaridade_media']:.3f}\")\n",
       "print(f\"   Score Final: {resultados_finetuned['score_final_medio']:.3f}\")\n",
       "print(f\"   Comprimento: {resultados_finetuned['comprimento_medio']:.1f} palavras\")\n",
       "\n",
       "# Calculando melhorias\n",
       "melhorias = {}\n",
       "for metrica in ['bleu_medio', 'similaridade_media', 'score_final_medio']:\n",
       "    base = resultados_base[metrica]\n",
       "    finetuned = resultados_finetuned[metrica]\n",
       "    if base > 0:\n",
       "        melhoria = ((finetuned - base) / base) * 100\n",
       "        melhorias[metrica] = melhoria\n",
       "\n",
       "print(f\"\\nüìà MELHORIAS:\")\n",
       "for metrica, melhoria in melhorias.items():\n",
       "    print(f\"   {metrica.replace('_', ' ').title()}: {melhoria:+.1f}%\")\n",
       "\n",
       "print(\"\\n‚úÖ Teste A/B conclu√≠do! Fine tuning melhorou o modelo!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **8. Salvando relat√≥rio de avalia√ß√£o**\n",
       "\n",
       "Vamos salvar um relat√≥rio completo:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Salvando relat√≥rio de avalia√ß√£o\n",
       "print(\"üìÑ SALVANDO RELAT√ìRIO DE AVALIA√á√ÉO\\n\")\n",
       "\n",
       "# Criando relat√≥rio\n",
       "relatorio = {\n",
       "    \"modelo\": model_name,\n",
       "    \"data_avaliacao\": \"2024-01-01\",\n",
       "    \"num_exemplos\": len(df_resultados),\n",
       "    \"metricas_gerais\": {\n",
       "        \"bleu_medio\": float(df_resultados['bleu'].mean()),\n",
       "        \"similaridade_media\": float(df_resultados['similaridade'].mean()),\n",
       "        \"score_final_medio\": float(df_resultados['score_final'].mean()),\n",
       "        \"comprimento_medio\": float(df_resultados['comprimento'].mean())\n",
       "    },\n",
       "    \"melhores_resultados\": df_resultados.nlargest(3, 'score_final').to_dict('records'),\n",
       "    \"piores_resultados\": df_resultados.nsmallest(3, 'score_final').to_dict('records'),\n",
       "    \"comparacao_ab\": {\n",
       "        \"modelo_base\": resultados_base,\n",
       "        \"modelo_finetuned\": resultados_finetuned,\n",
       "        \"melhorias\": melhorias\n",
       "    }\n",
       "}\n",
       "\n",
       "# Salvando em JSON\n",
       "with open('avaliacao_relatorio.json', 'w', encoding='utf-8') as f:\n",
       "    json.dump(relatorio, f, indent=2, ensure_ascii=False)\n",
       "\n",
       "print(\"‚úÖ Relat√≥rio salvo em 'avaliacao_relatorio.json'\")\n",
       "\n",
       "# Salvando resultados detalhados\n",
       "df_resultados.to_csv('avaliacao_resultados.csv', index=False)\n",
       "print(\"‚úÖ Resultados detalhados salvos em 'avaliacao_resultados.csv'\")\n",
       "\n",
       "print(\"\\nüìä RESUMO DO RELAT√ìRIO:\")\n",
       "print(f\"   Modelo: {relatorio['modelo']}\")\n",
       "print(f\"   Exemplos avaliados: {relatorio['num_exemplos']}\")\n",
       "print(f\"   Score final m√©dio: {relatorio['metricas_gerais']['score_final_medio']:.3f}\")\n",
       "print(f\"   Melhoria vs base: {relatorio['comparacao_ab']['melhorias'].get('score_final_medio', 0):+.1f}%\")\n",
       "\n",
       "print(\"\\nüéâ Relat√≥rio de avalia√ß√£o conclu√≠do!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **9. Teste R√°pido**\n",
       "\n",
       "Vamos testar seu entendimento sobre avalia√ß√£o:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Teste r√°pido sobre avalia√ß√£o\n",
       "print(\"üß™ TESTE R√ÅPIDO - AVALIA√á√ÉO\\n\")\n",
       "\n",
       "perguntas_teste = [\n",
       "    {\n",
       "        \"pergunta\": \"O que significa BLEU score?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) Medida de velocidade\", \n",
       "            \"B) Medida de similaridade de texto\", \n",
       "            \"C) Medida de mem√≥ria\"\n",
       "        ],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"BLEU score mede a similaridade entre texto gerado e texto de refer√™ncia!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"Qual √© o objetivo do teste A/B em avalia√ß√£o de modelos?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) Comparar dois modelos\", \n",
       "            \"B) Economizar recursos\", \n",
       "            \"C) Acelerar o treinamento\"\n",
       "        ],\n",
       "        \"resposta\": \"A\",\n",
       "        \"explicacao\": \"Teste A/B compara o desempenho de dois modelos diferentes!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"O que indica um score final alto?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) Modelo lento\", \n",
       "            \"B) Modelo com boa qualidade\", \n",
       "            \"C) Modelo com pouca mem√≥ria\"\n",
       "        ],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"Score final alto indica que o modelo tem boa qualidade nas respostas!\"\n",
       "    }\n",
       "]\n",
       "\n",
       "for i, q in enumerate(perguntas_teste, 1):\n",
       "    print(f\"‚ùì {i}. {q['pergunta']}\")\n",
       "    for opcao in q['opcoes']:\n",
       "        print(f\"   {opcao}\")\n",
       "    print(f\"üí° Resposta: {q['resposta']} - {q['explicacao']}\")\n",
       "    print()\n",
       "\n",
       "print(\"üéâ Parab√©ns! Voc√™ j√° entende os fundamentos da avalia√ß√£o de modelos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üéâ M√≥dulo 5 Conclu√≠do!**\n",
       "\n",
       "### **O que aprendemos:**\n",
       "\n",
       "‚úÖ **M√©tricas de avalia√ß√£o** (BLEU, Similaridade, Score composto)  \n",
       "‚úÖ **Avalia√ß√£o autom√°tica** de respostas  \n",
       "‚úÖ **An√°lise estat√≠stica** dos resultados  \n",
       "‚úÖ **Visualiza√ß√£o** dos resultados  \n",
       "‚úÖ **Teste A/B** comparando modelos  \n",
       "‚úÖ **Relat√≥rios de avalia√ß√£o** completos\n",
       "\n",
       "### **Pr√≥ximos Passos:**\n",
       "\n",
       "üöÄ **M√≥dulo 6**: Deploy como abrir um restaurante\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica do Instrutor**: Avaliar um modelo √© como corrigir uma prova - voc√™ precisa de crit√©rios claros e imparciais! Agora que sabemos que nossa IA aprendeu bem, vamos coloc√°-la para trabalhar! üòÑ\n",
       "\n",
       "**üöÄ Pr√≥ximo m√≥dulo**: Vamos fazer o deploy do nosso modelo treinado!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }