{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **Setup Inicial - Curso de Fine Tuning**\n",
       "\n",
       "## **Preparando o Terreno para Treinar IAs**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© este setup?**\n",
       "\n",
       "Imagine que voc√™ vai cozinhar um prato especial. Antes de come√ßar, voc√™ precisa:\n",
       "- ‚úÖ Ter todos os ingredientes\n",
       "- ‚úÖ Preparar a cozinha\n",
       "- ‚úÖ Verificar se o fog√£o funciona\n",
       "- ‚úÖ Ter as ferramentas certas\n",
       "\n",
       "**Fine Tuning √© a mesma coisa!** Precisamos preparar o \"ambiente de cozinha\" antes de come√ßar a \"cozinhar\" nossos modelos.\n",
       "\n",
       "**Por que este setup √© importante?**\n",
       "\n",
       "Fine Tuning √© como treinar um atleta de alto n√≠vel. Voc√™ precisa de:\n",
       "- **GPU** (como uma academia de ponta)\n",
       "- **Bibliotecas certas** (como equipamentos de qualidade)\n",
       "- **Configura√ß√µes adequadas** (como um plano de treino)\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Cozinheiro preparando ingredientes e ferramentas\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **1. Verificando se estamos no Colab**\n",
       "\n",
       "Primeiro, vamos garantir que estamos no ambiente certo:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Verificando se estamos no Google Colab\n",
       "try:\n",
       "    import google.colab\n",
       "    print(\"‚úÖ Estamos no Google Colab!\")\n",
       "    IN_COLAB = True\n",
       "except ImportError:\n",
       "    print(\"‚ö†Ô∏è  N√£o estamos no Google Colab. Algumas funcionalidades podem n√£o funcionar.\")\n",
       "    IN_COLAB = False"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **2. Instalando as Depend√™ncias**\n",
       "\n",
       "Agora vamos instalar todas as bibliotecas que precisamos:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Instalando as depend√™ncias principais\n",
       "print(\"üîß Instalando depend√™ncias...\")\n",
       "\n",
       "!pip install torch>=2.0.0 transformers>=4.30.0 datasets>=2.12.0\n",
       "!pip install accelerate>=0.20.0 peft>=0.4.0 bitsandbytes>=0.41.0 trl>=0.7.0\n",
       "!pip install langchain>=0.1.0 langchain-community>=0.0.10 langchain-core>=0.1.0\n",
       "!pip install huggingface_hub>=0.19.0 python-dotenv>=1.0.0\n",
       "!pip install numpy>=1.24.0 pandas>=2.0.0 matplotlib>=3.7.0 seaborn>=0.12.0\n",
       "!pip install scikit-learn>=1.3.0 gradio>=3.40.0 wandb>=0.15.0\n",
       "\n",
       "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **3. Configurando GPU (se dispon√≠vel)**\n",
       "\n",
       "Fine Tuning √© como correr uma maratona - voc√™ precisa de energia (GPU) para n√£o ficar exausto:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Verificando e configurando GPU\n",
       "import torch\n",
       "\n",
       "print(\"üîç Verificando disponibilidade de GPU...\")\n",
       "\n",
       "if torch.cuda.is_available():\n",
       "    print(f\"‚úÖ GPU dispon√≠vel: {torch.cuda.get_device_name(0)}\")\n",
       "    print(f\"   Mem√≥ria total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
       "    print(f\"   Mem√≥ria livre: {torch.cuda.memory_allocated(0) / 1e9:.1f} GB\")\n",
       "    DEVICE = \"cuda\"\n",
       "else:\n",
       "    print(\"‚ö†Ô∏è  GPU n√£o dispon√≠vel. Vamos usar CPU (ser√° mais lento, mas funciona!)\")\n",
       "    DEVICE = \"cpu\"\n",
       "\n",
       "print(f\"üéØ Dispositivo selecionado: {DEVICE}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **4. Configurando Hugging Face**\n",
       "\n",
       "Vamos configurar o acesso ao Hugging Face (nossa \"biblioteca de modelos\"):"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configurando Hugging Face\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "# Carregando vari√°veis de ambiente\n",
       "load_dotenv()\n",
       "\n",
       "# Verificando token do Hugging Face\n",
       "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
       "\n",
       "if hf_token:\n",
       "    print(\"‚úÖ Token do Hugging Face configurado!\")\n",
       "else:\n",
       "    print(\"‚ö†Ô∏è  Token do Hugging Face n√£o encontrado.\")\n",
       "    print(\"üí° Para obter um token gratuito:\")\n",
       "    print(\"   1. Acesse: https://huggingface.co/settings/tokens\")\n",
       "    print(\"   2. Crie um novo token\")\n",
       "    print(\"   3. Adicione como vari√°vel de ambiente HUGGINGFACEHUB_API_TOKEN\")\n",
       "    print(\"\\nüîß Por enquanto, vamos continuar sem o token (algumas funcionalidades ser√£o limitadas)\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **5. Testando as Importa√ß√µes**\n",
       "\n",
       "Vamos verificar se tudo est√° funcionando:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Testando importa√ß√µes principais\n",
       "print(\"üß™ Testando importa√ß√µes...\")\n",
       "\n",
       "try:\n",
       "    import transformers\n",
       "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
       "except ImportError as e:\n",
       "    print(f\"‚ùå Erro ao importar transformers: {e}\")\n",
       "\n",
       "try:\n",
       "    import torch\n",
       "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
       "except ImportError as e:\n",
       "    print(f\"‚ùå Erro ao importar torch: {e}\")\n",
       "\n",
       "try:\n",
       "    import datasets\n",
       "    print(f\"‚úÖ Datasets: {datasets.__version__}\")\n",
       "except ImportError as e:\n",
       "    print(f\"‚ùå Erro ao importar datasets: {e}\")\n",
       "\n",
       "try:\n",
       "    import peft\n",
       "    print(f\"‚úÖ PEFT: {peft.__version__}\")\n",
       "except ImportError as e:\n",
       "    print(f\"‚ùå Erro ao importar PEFT: {e}\")\n",
       "\n",
       "try:\n",
       "    import accelerate\n",
       "    print(f\"‚úÖ Accelerate: {accelerate.__version__}\")\n",
       "except ImportError as e:\n",
       "    print(f\"‚ùå Erro ao importar accelerate: {e}\")\n",
       "\n",
       "print(\"\\nüéâ Setup conclu√≠do com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **6. Fun√ß√£o Helper para LLMs**\n",
       "\n",
       "Vamos criar uma fun√ß√£o que nos ajuda a escolher o melhor LLM dispon√≠vel:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def get_llm_colab():\n",
       "    \"\"\"\n",
       "    Retorna o melhor LLM dispon√≠vel no Colab\n",
       "    \n",
       "    √â como escolher o melhor funcion√°rio dispon√≠vel para a tarefa!\n",
       "    \"\"\"\n",
       "    \n",
       "    # Tentativa 1: OpenAI (se tiver API key)\n",
       "    try:\n",
       "        from langchain_openai import ChatOpenAI\n",
       "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
       "        if api_key:\n",
       "            print(\"ü§ñ Usando OpenAI GPT-3.5-turbo\")\n",
       "            return ChatOpenAI(\n",
       "                model=\"gpt-3.5-turbo\",\n",
       "                temperature=0.7,\n",
       "                api_key=api_key\n",
       "            )\n",
       "    except Exception as e:\n",
       "        print(f\"‚ö†Ô∏è  OpenAI n√£o dispon√≠vel: {e}\")\n",
       "    \n",
       "    # Tentativa 2: Hugging Face (gratuito)\n",
       "    try:\n",
       "        from langchain_community.llms import HuggingFaceHub\n",
       "        token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
       "        if token:\n",
       "            print(\"ü§ñ Usando Hugging Face (gratuito)\")\n",
       "            return HuggingFaceHub(\n",
       "                repo_id=\"google/flan-t5-base\",\n",
       "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
       "            )\n",
       "    except Exception as e:\n",
       "        print(f\"‚ö†Ô∏è  Hugging Face n√£o dispon√≠vel: {e}\")\n",
       "    \n",
       "    # Fallback: Modelo local simples\n",
       "    print(\"ü§ñ Usando modelo local simples\")\n",
       "    return None\n",
       "\n",
       "# Testando a fun√ß√£o\n",
       "print(\"üß™ Testando fun√ß√£o de LLM...\")\n",
       "llm = get_llm_colab()\n",
       "if llm:\n",
       "    print(\"‚úÖ LLM configurado com sucesso!\")\n",
       "else:\n",
       "    print(\"‚ö†Ô∏è  LLM n√£o configurado. Vamos usar modelos locais no curso.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **7. Criando Estrutura de Pastas**\n",
       "\n",
       "Vamos organizar nosso \"espa√ßo de trabalho\":"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando estrutura de pastas\n",
       "import os\n",
       "\n",
       "pastas = [\"datasets\", \"models\", \"checkpoints\", \"logs\"]\n",
       "\n",
       "for pasta in pastas:\n",
       "    if not os.path.exists(pasta):\n",
       "        os.makedirs(pasta)\n",
       "        print(f\"üìÅ Criada pasta: {pasta}\")\n",
       "    else:\n",
       "        print(f\"üìÅ Pasta j√° existe: {pasta}\")\n",
       "\n",
       "print(\"\\nüèóÔ∏è  Estrutura de pastas criada!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **8. Teste Final - Hello World do Fine Tuning**\n",
       "\n",
       "Vamos fazer um teste r√°pido para garantir que tudo est√° funcionando:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Teste final - Carregando um modelo pequeno\n",
       "print(\"üß™ Teste final - Carregando modelo...\")\n",
       "\n",
       "try:\n",
       "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "    \n",
       "    # Carregando um modelo pequeno para teste\n",
       "    model_name = \"microsoft/DialoGPT-small\"\n",
       "    \n",
       "    print(f\"üì• Baixando modelo: {model_name}\")\n",
       "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "    \n",
       "    print(\"‚úÖ Modelo carregado com sucesso!\")\n",
       "    print(f\"   Par√¢metros: {model.num_parameters():,}\")\n",
       "    \n",
       "    # Teste simples\n",
       "    input_text = \"Ol√°, como voc√™ est√°?\"\n",
       "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
       "    \n",
       "    print(f\"‚úÖ Tokeniza√ß√£o funcionando!\")\n",
       "    print(f\"   Input: {input_text}\")\n",
       "    print(f\"   Tokens: {inputs['input_ids'].shape}\")\n",
       "    \n",
       "except Exception as e:\n",
       "    print(f\"‚ùå Erro no teste final: {e}\")\n",
       "    print(\"üí° N√£o se preocupe, vamos resolver isso nos pr√≥ximos m√≥dulos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üéâ Setup Conclu√≠do!**\n",
       "\n",
       "### **O que fizemos:**\n",
       "\n",
       "‚úÖ **Verificamos o ambiente** (Colab + GPU)  \n",
       "‚úÖ **Instalamos todas as depend√™ncias**  \n",
       "‚úÖ **Configuramos Hugging Face**  \n",
       "‚úÖ **Testamos as importa√ß√µes**  \n",
       "‚úÖ **Criamos estrutura de pastas**  \n",
       "‚úÖ **Fizemos teste final**\n",
       "\n",
       "### **Pr√≥ximos Passos:**\n",
       "\n",
       "üöÄ **M√≥dulo 1**: Entendendo Fine Tuning  \n",
       "üöÄ **M√≥dulo 2**: Preparando Dados  \n",
       "üöÄ **M√≥dulo 3**: Escolhendo Modelos  \n",
       "üöÄ **M√≥dulo 4**: Treinando  \n",
       "üöÄ **M√≥dulo 5**: Avaliando  \n",
       "üöÄ **M√≥dulo 6**: Deploy\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica do Instrutor**: Se algo deu errado, n√£o se preocupe! Fine Tuning √© como cozinhar - √†s vezes a primeira tentativa n√£o sai perfeita, mas com pr√°tica voc√™ vira um chef! üòÑ\n",
       "\n",
       "**üöÄ Pr√≥ximo m√≥dulo**: Vamos entender o que √© Fine Tuning e por que voc√™ precisa saber!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }