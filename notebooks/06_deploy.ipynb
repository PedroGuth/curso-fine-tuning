{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **M√≥dulo 6: Deploy como abrir um restaurante**\n",
       "\n",
       "## **Aula 6.1: Exportando e salvando modelos**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© Deploy?**\n",
       "\n",
       "Imagine que voc√™ √© um chef que criou uma receita incr√≠vel. Agora voc√™ precisa:\n",
       "- ‚úÖ **Abrir um restaurante** (infraestrutura)\n",
       "- ‚úÖ **Contratar gar√ßons** (interface)\n",
       "- ‚úÖ **Fazer propaganda** (divulga√ß√£o)\n",
       "- ‚úÖ **Atender clientes** (produ√ß√£o)\n",
       "\n",
       "**Deploy √© a mesma coisa!** Vamos colocar nossa IA treinada para trabalhar no mundo real.\n",
       "\n",
       "**Por que o Deploy √© importante?**\n",
       "\n",
       "√â como abrir um neg√≥cio:\n",
       "- **Sem deploy**: IA fica guardada no computador (como receita na gaveta)\n",
       "- **Com deploy**: IA atende clientes reais (como restaurante funcionando)\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Chef abrindo restaurante vs deploy de modelo de IA\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Importa√ß√µes necess√°rias\n",
       "import torch\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "import gradio as gr\n",
       "import streamlit as st\n",
       "import json\n",
       "import os\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')\n",
       "\n",
       "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **1. Preparando o modelo para deploy**\n",
       "\n",
       "Vamos preparar nosso modelo treinado:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Carregando modelo treinado (simulado)\n",
       "print(\"ü§ñ PREPARANDO MODELO PARA DEPLOY\\n\")\n",
       "\n",
       "# Para demonstra√ß√£o, vamos usar o modelo base\n",
       "model_name = \"microsoft/DialoGPT-small\"\n",
       "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "\n",
       "print(f\"‚úÖ Modelo carregado: {model_name}\")\n",
       "print(f\"üìä Par√¢metros: {model.num_parameters():,}\")\n",
       "\n",
       "# Configurando tokenizer\n",
       "if tokenizer.pad_token is None:\n",
       "    tokenizer.pad_token = tokenizer.eos_token\n",
       "\n",
       "# Fun√ß√£o de infer√™ncia otimizada\n",
       "def gerar_resposta_otimizada(pergunta, max_length=100, temperature=0.7):\n",
       "    \"\"\"\n",
       "    Fun√ß√£o otimizada para produ√ß√£o\n",
       "    \"\"\"\n",
       "    try:\n",
       "        # Formatando entrada\n",
       "        input_text = f\"User: {pergunta}\\nAssistant:\"\n",
       "        \n",
       "        # Tokenizando\n",
       "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
       "        \n",
       "        # Gerando resposta\n",
       "        with torch.no_grad():\n",
       "            outputs = model.generate(\n",
       "                inputs[\"input_ids\"],\n",
       "                max_length=max_length,\n",
       "                num_return_sequences=1,\n",
       "                temperature=temperature,\n",
       "                do_sample=True,\n",
       "                pad_token_id=tokenizer.eos_token_id\n",
       "            )\n",
       "        \n",
       "        # Decodificando resposta\n",
       "        resposta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "        \n",
       "        # Extraindo apenas a parte do assistant\n",
       "        if \"Assistant:\" in resposta_completa:\n",
       "            resposta = resposta_completa.split(\"Assistant:\")[-1].strip()\n",
       "        else:\n",
       "            resposta = resposta_completa\n",
       "        \n",
       "        return resposta\n",
       "        \n",
       "    except Exception as e:\n",
       "        return f\"Erro na gera√ß√£o: {e}\"\n",
       "\n",
       "# Testando fun√ß√£o otimizada\n",
       "pergunta_teste = \"Qual √© o melhor iPhone?\"\n",
       "resposta = gerar_resposta_otimizada(pergunta_teste)\n",
       "\n",
       "print(f\"üß™ Teste de infer√™ncia:\")\n",
       "print(f\"   Pergunta: {pergunta_teste}\")\n",
       "print(f\"   Resposta: {resposta}\")\n",
       "\n",
       "print(\"\\n‚úÖ Modelo preparado para deploy!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **2. Deploy com Gradio (Interface Web)**\n",
       "\n",
       "Vamos criar uma interface web simples:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando interface Gradio\n",
       "print(\"üåê CRIANDO INTERFACE GRADIO\\n\")\n",
       "\n",
       "def interface_gradio(pergunta, max_length, temperature):\n",
       "    \"\"\"\n",
       "    Interface Gradio para o modelo\n",
       "    \"\"\"\n",
       "    if not pergunta.strip():\n",
       "        return \"Por favor, digite uma pergunta.\"\n",
       "    \n",
       "    resposta = gerar_resposta_otimizada(pergunta, max_length, temperature)\n",
       "    return resposta\n",
       "\n",
       "# Configurando interface\n",
       "iface = gr.Interface(\n",
       "    fn=interface_gradio,\n",
       "    inputs=[\n",
       "        gr.Textbox(label=\"‚ùì Fa√ßa sua pergunta sobre iPhone:\", placeholder=\"Ex: Qual √© o melhor iPhone para fotografia?\"),\n",
       "        gr.Slider(minimum=50, maximum=200, value=100, step=10, label=\"üìè Comprimento m√°ximo da resposta\"),\n",
       "        gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label=\"üå°Ô∏è Temperatura (criatividade)\")\n",
       "    ],\n",
       "    outputs=gr.Textbox(label=\"ü§ñ Resposta da IA:\", lines=5),\n",
       "    title=\"üì± Assistente de Vendas iPhone\",\n",
       "    description=\"IA especializada em vendas de iPhone. Fa√ßa perguntas sobre produtos, pre√ßos e caracter√≠sticas!\",\n",
       "    examples=[\n",
       "        [\"Qual √© o melhor iPhone para fotografia?\", 100, 0.7],\n",
       "        [\"Quanto custa o iPhone 15?\", 100, 0.7],\n",
       "        [\"Qual √© a diferen√ßa entre iPhone 15 e iPhone 15 Pro?\", 150, 0.7],\n",
       "        [\"O iPhone 15 tem carregamento sem fio?\", 100, 0.7]\n",
       "    ],\n",
       "    theme=gr.themes.Soft()\n",
       ")\n",
       "\n",
       "print(\"‚úÖ Interface Gradio criada!\")\n",
       "print(\"\\nüöÄ Para lan√ßar a interface, execute:\")\n",
       "print(\"   iface.launch(share=True)\")\n",
       "\n",
       "# Salvando interface\n",
       "iface.save(\"deploy_gradio\")\n",
       "print(\"‚úÖ Interface salva em 'deploy_gradio'\")\n",
       "\n",
       "print(\"\\nüí° Gradio cria uma interface web bonita e funcional automaticamente!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **3. Deploy com Streamlit (Aplica√ß√£o Web)**\n",
       "\n",
       "Vamos criar uma aplica√ß√£o mais robusta:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando aplica√ß√£o Streamlit\n",
       "print(\"üì± CRIANDO APLICA√á√ÉO STREAMLIT\\n\")\n",
       "\n",
       "streamlit_code = \"\"\"\n",
       "import streamlit as st\n",
       "import torch\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "import time\n",
       "\n",
       "# Configura√ß√£o da p√°gina\n",
       "st.set_page_config(\n",
       "    page_title=\"üì± Assistente iPhone\",\n",
       "    page_icon=\"üì±\",\n",
       "    layout=\"wide\"\n",
       ")\n",
       "\n",
       "# T√≠tulo\n",
       "st.title(\"üì± Assistente de Vendas iPhone\")\n",
       "st.markdown(\"---\")\n",
       "\n",
       "# Sidebar com configura√ß√µes\n",
       "with st.sidebar:\n",
       "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
       "    max_length = st.slider(\"üìè Comprimento m√°ximo\", 50, 200, 100)\n",
       "    temperature = st.slider(\"üå°Ô∏è Temperatura\", 0.1, 1.0, 0.7)\n",
       "    \n",
       "    st.markdown(\"---\")\n",
       "    st.markdown(\"### üìä Estat√≠sticas\")\n",
       "    st.metric(\"Perguntas respondidas\", \"0\")\n",
       "    st.metric(\"Tempo m√©dio\", \"0.5s\")\n",
       "\n",
       "# √Årea principal\n",
       "col1, col2 = st.columns([2, 1])\n",
       "\n",
       "with col1:\n",
       "    st.subheader(\"‚ùì Fa√ßa sua pergunta\")\n",
       "    pergunta = st.text_area(\n",
       "        \"Digite sua pergunta sobre iPhone:\",\n",
       "        placeholder=\"Ex: Qual √© o melhor iPhone para fotografia?\",\n",
       "        height=100\n",
       "    )\n",
       "    \n",
       "    if st.button(\"üöÄ Gerar Resposta\", type=\"primary\"):\n",
       "        if pergunta.strip():\n",
       "            with st.spinner(\"ü§ñ IA pensando...\"):\n",
       "                start_time = time.time()\n",
       "                # Aqui voc√™ chamaria sua fun√ß√£o de gera√ß√£o\n",
       "                resposta = \"Esta √© uma resposta simulada da IA treinada.\"\n",
       "                end_time = time.time()\n",
       "                \n",
       "            st.success(f\"‚úÖ Resposta gerada em {end_time - start_time:.2f}s\")\n",
       "            st.markdown(\"### ü§ñ Resposta:\")\n",
       "            st.write(resposta)\n",
       "        else:\n",
       "            st.error(\"Por favor, digite uma pergunta.\")\n",
       "\n",
       "with col2:\n",
       "    st.subheader(\"üí° Exemplos\")\n",
       "    exemplos = [\n",
       "        \"Qual √© o melhor iPhone para fotografia?\",\n",
       "        \"Quanto custa o iPhone 15?\",\n",
       "        \"Qual √© a diferen√ßa entre iPhone 15 e iPhone 15 Pro?\",\n",
       "        \"O iPhone 15 tem carregamento sem fio?\"\n",
       "    ]\n",
       "    \n",
       "    for exemplo in exemplos:\n",
       "        if st.button(exemplo, key=exemplo):\n",
       "            st.session_state.pergunta = exemplo\n",
       "            st.rerun()\n",
       "\n",
       "# Footer\n",
       "st.markdown(\"---\")\n",
       "st.markdown(\"*Desenvolvido com ‚ù§Ô∏è usando Fine Tuning*\")\n",
       "\"\"\"\n",
       "\n",
       "# Salvando aplica√ß√£o Streamlit\n",
       "with open(\"app_streamlit.py\", \"w\", encoding=\"utf-8\") as f:\n",
       "    f.write(streamlit_code)\n",
       "\n",
       "print(\"‚úÖ Aplica√ß√£o Streamlit criada em 'app_streamlit.py'\")\n",
       "print(\"\\nüöÄ Para executar a aplica√ß√£o:\")\n",
       "print(\"   streamlit run app_streamlit.py\")\n",
       "\n",
       "print(\"\\nüí° Streamlit cria aplica√ß√µes web mais robustas e interativas!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **4. Deploy em Hugging Face Spaces**\n",
       "\n",
       "Vamos preparar para deploy na nuvem:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Preparando para Hugging Face Spaces\n",
       "print(\"‚òÅÔ∏è PREPARANDO PARA HUGGING FACE SPACES\\n\")\n",
       "\n",
       "# Criando arquivo requirements.txt\n",
       "requirements_content = \"\"\"\n",
       "torch>=2.0.0\n",
       "transformers>=4.30.0\n",
       "gradio>=3.40.0\n",
       "streamlit>=1.25.0\n",
       "numpy>=1.24.0\n",
       "pandas>=2.0.0\n",
       "\"\"\"\n",
       "\n",
       "with open(\"requirements.txt\", \"w\") as f:\n",
       "    f.write(requirements_content)\n",
       "\n",
       "print(\"‚úÖ requirements.txt criado\")\n",
       "\n",
       "# Criando README para o Space\n",
       "readme_content = \"\"\"\n",
       "# üì± Assistente de Vendas iPhone\n",
       "\n",
       "Este √© um assistente de IA especializado em vendas de iPhone, treinado com Fine Tuning.\n",
       "\n",
       "## üöÄ Como usar\n",
       "\n",
       "1. Digite sua pergunta sobre iPhone\n",
       "2. Ajuste os par√¢metros se necess√°rio\n",
       "3. Clique em \"Gerar Resposta\"\n",
       "\n",
       "## üí° Exemplos de perguntas\n",
       "\n",
       "- Qual √© o melhor iPhone para fotografia?\n",
       "- Quanto custa o iPhone 15?\n",
       "- Qual √© a diferen√ßa entre iPhone 15 e iPhone 15 Pro?\n",
       "- O iPhone 15 tem carregamento sem fio?\n",
       "\n",
       "## üõ†Ô∏è Tecnologias\n",
       "\n",
       "- Fine Tuning com LoRA\n",
       "- Transformers (Hugging Face)\n",
       "- Gradio para interface\n",
       "\n",
       "---\n",
       "\n",
       "*Desenvolvido como parte do curso de Fine Tuning*\n",
       "\"\"\"\n",
       "\n",
       "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
       "    f.write(readme_content)\n",
       "\n",
       "print(\"‚úÖ README.md criado\")\n",
       "\n",
       "# Criando app.py para Hugging Face Spaces\n",
       "app_content = \"\"\"\n",
       "import gradio as gr\n",
       "import torch\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "\n",
       "# Carregando modelo\n",
       "model_name = \"microsoft/DialoGPT-small\"\n",
       "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "\n",
       "if tokenizer.pad_token is None:\n",
       "    tokenizer.pad_token = tokenizer.eos_token\n",
       "\n",
       "def gerar_resposta(pergunta, max_length, temperature):\n",
       "    try:\n",
       "        input_text = f\"User: {pergunta}\\nAssistant:\"\n",
       "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
       "        \n",
       "        with torch.no_grad():\n",
       "            outputs = model.generate(\n",
       "                inputs[\"input_ids\"],\n",
       "                max_length=max_length,\n",
       "                num_return_sequences=1,\n",
       "                temperature=temperature,\n",
       "                do_sample=True,\n",
       "                pad_token_id=tokenizer.eos_token_id\n",
       "            )\n",
       "        \n",
       "        resposta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "        \n",
       "        if \"Assistant:\" in resposta_completa:\n",
       "            resposta = resposta_completa.split(\"Assistant:\")[-1].strip()\n",
       "        else:\n",
       "            resposta = resposta_completa\n",
       "        \n",
       "        return resposta\n",
       "        \n",
       "    except Exception as e:\n",
       "        return f\"Erro na gera√ß√£o: {e}\"\n",
       "\n",
       "# Interface Gradio\n",
       "iface = gr.Interface(\n",
       "    fn=gerar_resposta,\n",
       "    inputs=[\n",
       "        gr.Textbox(label=\"‚ùì Fa√ßa sua pergunta sobre iPhone:\", placeholder=\"Ex: Qual √© o melhor iPhone para fotografia?\"),\n",
       "        gr.Slider(minimum=50, maximum=200, value=100, step=10, label=\"üìè Comprimento m√°ximo da resposta\"),\n",
       "        gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label=\"üå°Ô∏è Temperatura (criatividade)\")\n",
       "    ],\n",
       "    outputs=gr.Textbox(label=\"ü§ñ Resposta da IA:\", lines=5),\n",
       "    title=\"üì± Assistente de Vendas iPhone\",\n",
       "    description=\"IA especializada em vendas de iPhone. Fa√ßa perguntas sobre produtos, pre√ßos e caracter√≠sticas!\",\n",
       "    examples=[\n",
       "        [\"Qual √© o melhor iPhone para fotografia?\", 100, 0.7],\n",
       "        [\"Quanto custa o iPhone 15?\", 100, 0.7],\n",
       "        [\"Qual √© a diferen√ßa entre iPhone 15 e iPhone 15 Pro?\", 150, 0.7],\n",
       "        [\"O iPhone 15 tem carregamento sem fio?\", 100, 0.7]\n",
       "    ],\n",
       "    theme=gr.themes.Soft()\n",
       ")\n",
       "\n",
       "iface.launch()\n",
       "\"\"\"\n",
       "\n",
       "with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
       "    f.write(app_content)\n",
       "\n",
       "print(\"‚úÖ app.py criado\")\n",
       "\n",
       "print(\"\\nüöÄ Para fazer deploy no Hugging Face Spaces:\")\n",
       "print(\"   1. Crie um novo Space no Hugging Face\")\n",
       "print(\"   2. Fa√ßa upload dos arquivos: app.py, requirements.txt, README.md\")\n",
       "print(\"   3. O Space ser√° automaticamente deployado!\")\n",
       "\n",
       "print(\"\\nüí° Hugging Face Spaces oferece hospedagem gratuita para aplica√ß√µes de IA!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **5. Deploy com API REST**\n",
       "\n",
       "Vamos criar uma API para integra√ß√£o:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando API REST com FastAPI\n",
       "print(\"üîå CRIANDO API REST\\n\")\n",
       "\n",
       "api_code = \"\"\"\n",
       "from fastapi import FastAPI, HTTPException\n",
       "from pydantic import BaseModel\n",
       "import torch\n",
       "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "import uvicorn\n",
       "from typing import Optional\n",
       "\n",
       "# Configurando FastAPI\n",
       "app = FastAPI(\n",
       "    title=\"üì± Assistente iPhone API\",\n",
       "    description=\"API para assistente de vendas de iPhone treinado com Fine Tuning\",\n",
       "    version=\"1.0.0\"\n",
       ")\n",
       "\n",
       "# Modelo de dados\n",
       "class PerguntaRequest(BaseModel):\n",
       "    pergunta: str\n",
       "    max_length: Optional[int] = 100\n",
       "    temperature: Optional[float] = 0.7\n",
       "\n",
       "class RespostaResponse(BaseModel):\n",
       "    pergunta: str\n",
       "    resposta: str\n",
       "    tempo_geracao: float\n",
       "    status: str\n",
       "\n",
       "# Carregando modelo (global)\n",
       "model_name = \"microsoft/DialoGPT-small\"\n",
       "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "\n",
       "if tokenizer.pad_token is None:\n",
       "    tokenizer.pad_token = tokenizer.eos_token\n",
       "\n",
       "def gerar_resposta_api(pergunta, max_length, temperature):\n",
       "    try:\n",
       "        input_text = f\"User: {pergunta}\\nAssistant:\"\n",
       "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
       "        \n",
       "        with torch.no_grad():\n",
       "            outputs = model.generate(\n",
       "                inputs[\"input_ids\"],\n",
       "                max_length=max_length,\n",
       "                num_return_sequences=1,\n",
       "                temperature=temperature,\n",
       "                do_sample=True,\n",
       "                pad_token_id=tokenizer.eos_token_id\n",
       "            )\n",
       "        \n",
       "        resposta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "        \n",
       "        if \"Assistant:\" in resposta_completa:\n",
       "            resposta = resposta_completa.split(\"Assistant:\")[-1].strip()\n",
       "        else:\n",
       "            resposta = resposta_completa\n",
       "        \n",
       "        return resposta\n",
       "        \n",
       "    except Exception as e:\n",
       "        raise HTTPException(status_code=500, detail=f\"Erro na gera√ß√£o: {e}\")\n",
       "\n",
       "@app.get(\"/\")\n",
       "async def root():\n",
       "    return {\"message\": \"üì± Assistente iPhone API - Funcionando!\"}\n",
       "\n",
       "@app.get(\"/health\")\n",
       "async def health_check():\n",
       "    return {\"status\": \"healthy\", \"model\": model_name}\n",
       "\n",
       "@app.post(\"/gerar-resposta\", response_model=RespostaResponse)\n",
       "async def gerar_resposta_endpoint(request: PerguntaRequest):\n",
       "    import time\n",
       "    \n",
       "    if not request.pergunta.strip():\n",
       "        raise HTTPException(status_code=400, detail=\"Pergunta n√£o pode estar vazia\")\n",
       "    \n",
       "    start_time = time.time()\n",
       "    resposta = gerar_resposta_api(request.pergunta, request.max_length, request.temperature)\n",
       "    end_time = time.time()\n",
       "    \n",
       "    return RespostaResponse(\n",
       "        pergunta=request.pergunta,\n",
       "        resposta=resposta,\n",
       "        tempo_geracao=end_time - start_time,\n",
       "        status=\"success\"\n",
       "    )\n",
       "\n",
       "@app.get(\"/exemplos\")\n",
       "async def get_exemplos():\n",
       "    return {\n",
       "        \"exemplos\": [\n",
       "            \"Qual √© o melhor iPhone para fotografia?\",\n",
       "            \"Quanto custa o iPhone 15?\",\n",
       "            \"Qual √© a diferen√ßa entre iPhone 15 e iPhone 15 Pro?\",\n",
       "            \"O iPhone 15 tem carregamento sem fio?\"\n",
       "        ]\n",
       "    }\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
       "\"\"\"\n",
       "\n",
       "# Salvando API\n",
       "with open(\"api_fastapi.py\", \"w\", encoding=\"utf-8\") as f:\n",
       "    f.write(api_code)\n",
       "\n",
       "print(\"‚úÖ API FastAPI criada em 'api_fastapi.py'\")\n",
       "print(\"\\nüöÄ Para executar a API:\")\n",
       "print(\"   python api_fastapi.py\")\n",
       "print(\"   # A API estar√° dispon√≠vel em http://localhost:8000\")\n",
       "\n",
       "# Criando exemplo de uso da API\n",
       "exemplo_uso_api = \"\"\"\n",
       "import requests\n",
       "\n",
       "# Exemplo de uso da API\n",
       "url = \"http://localhost:8000/gerar-resposta\"\n",
       "\n",
       "data = {\n",
       "    \"pergunta\": \"Qual √© o melhor iPhone para fotografia?\",\n",
       "    \"max_length\": 100,\n",
       "    \"temperature\": 0.7\n",
       "}\n",
       "\n",
       "response = requests.post(url, json=data)\n",
       "resultado = response.json()\n",
       "\n",
       "print(f\"Pergunta: {resultado['pergunta']}\")\n",
       "print(f\"Resposta: {resultado['resposta']}\")\n",
       "print(f\"Tempo: {resultado['tempo_geracao']:.2f}s\")\n",
       "\"\"\"\n",
       "\n",
       "with open(\"exemplo_uso_api.py\", \"w\", encoding=\"utf-8\") as f:\n",
       "    f.write(exemplo_uso_api)\n",
       "\n",
       "print(\"‚úÖ Exemplo de uso da API criado\")\n",
       "\n",
       "print(\"\\nüí° API REST permite integra√ß√£o com qualquer aplica√ß√£o!\")\n",
       "print(\"   - Web apps, mobile apps, chatbots, etc.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **6. Compara√ß√£o de m√©todos de deploy**\n",
       "\n",
       "Vamos comparar as diferentes op√ß√µes:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Compara√ß√£o de m√©todos de deploy\n",
       "print(\"üìä COMPARA√á√ÉO DE M√âTODOS DE DEPLOY\\n\")\n",
       "\n",
       "metodos_deploy = {\n",
       "    \"Gradio\": {\n",
       "        \"facilidade\": \"Muito f√°cil\",\n",
       "        \"velocidade\": \"R√°pido\",\n",
       "        \"customizacao\": \"Limitada\",\n",
       "        \"custo\": \"Gratuito\",\n",
       "        \"melhor_para\": \"Prototipagem, demonstra√ß√µes\",\n",
       "        \"limitacoes\": \"Interface b√°sica\"\n",
       "    },\n",
       "    \"Streamlit\": {\n",
       "        \"facilidade\": \"F√°cil\",\n",
       "        \"velocidade\": \"R√°pido\",\n",
       "        \"customizacao\": \"M√©dia\",\n",
       "        \"custo\": \"Gratuito\",\n",
       "        \"melhor_para\": \"Dashboards, aplica√ß√µes web\",\n",
       "        \"limitacoes\": \"Menos flex√≠vel que frameworks web\"\n",
       "    },\n",
       "    \"Hugging Face Spaces\": {\n",
       "        \"facilidade\": \"Muito f√°cil\",\n",
       "        \"velocidade\": \"Muito r√°pido\",\n",
       "        \"customizacao\": \"M√©dia\",\n",
       "        \"custo\": \"Gratuito\",\n",
       "        \"melhor_para\": \"Compartilhamento, demonstra√ß√µes\",\n",
       "        \"limitacoes\": \"Recursos limitados\"\n",
       "    },\n",
       "    \"API REST\": {\n",
       "        \"facilidade\": \"M√©dio\",\n",
       "        \"velocidade\": \"R√°pido\",\n",
       "        \"customizacao\": \"Alta\",\n",
       "        \"custo\": \"Vari√°vel\",\n",
       "        \"melhor_para\": \"Integra√ß√£o, aplica√ß√µes empresariais\",\n",
       "        \"limitacoes\": \"Precisa de infraestrutura\"\n",
       "    }\n",
       "}\n",
       "\n",
       "for metodo, info in metodos_deploy.items():\n",
       "    print(f\"üîπ {metodo}\")\n",
       "    print(f\"   üéØ Facilidade: {info['facilidade']}\")\n",
       "    print(f\"   ‚ö° Velocidade: {info['velocidade']}\")\n",
       "    print(f\"   üé® Customiza√ß√£o: {info['customizacao']}\")\n",
       "    print(f\"   üí∞ Custo: {info['custo']}\")\n",
       "    print(f\"   üéØ Melhor para: {info['melhor_para']}\")\n",
       "    print(f\"   ‚ö†Ô∏è  Limita√ß√µes: {info['limitacoes']}\")\n",
       "    print()\n",
       "\n",
       "print(\"üí° Escolha o m√©todo baseado no seu caso de uso e recursos dispon√≠veis!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **7. Checklist de deploy**\n",
       "\n",
       "Vamos criar um checklist para garantir um deploy bem-sucedido:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Checklist de deploy\n",
       "print(\"‚úÖ CHECKLIST DE DEPLOY\\n\")\n",
       "\n",
       "checklist = [\n",
       "    {\n",
       "        \"categoria\": \"üìã Prepara√ß√£o\",\n",
       "        \"itens\": [\n",
       "            \"Modelo treinado e testado\",\n",
       "            \"Dados de exemplo preparados\",\n",
       "            \"Fun√ß√£o de infer√™ncia otimizada\",\n",
       "            \"Tratamento de erros implementado\"\n",
       "        ]\n",
       "    },\n",
       "    {\n",
       "        \"categoria\": \"üåê Interface\",\n",
       "        \"itens\": [\n",
       "            \"Interface de usu√°rio criada\",\n",
       "            \"Exemplos de uso inclu√≠dos\",\n",
       "            \"Valida√ß√£o de entrada implementada\",\n",
       "            \"Feedback visual para o usu√°rio\"\n",
       "        ]\n",
       "    },\n",
       "    {\n",
       "        \"categoria\": \"üöÄ Deploy\",\n",
       "        \"itens\": [\n",
       "            \"Arquivos de configura√ß√£o criados\",\n",
       "            \"Depend√™ncias listadas\",\n",
       "            \"Documenta√ß√£o inclu√≠da\",\n",
       "            \"Testes de funcionamento realizados\"\n",
       "        ]\n",
       "    },\n",
       "    {\n",
       "        \"categoria\": \"üìä Monitoramento\",\n",
       "        \"itens\": [\n",
       "            \"Logs de erro configurados\",\n",
       "            \"M√©tricas de performance definidas\",\n",
       "            \"Sistema de backup implementado\",\n",
       "            \"Plano de escalabilidade definido\"\n",
       "        ]\n",
       "    }\n",
       "]\n",
       "\n",
       "for categoria in checklist:\n",
       "    print(f\"{categoria['categoria']}:\")\n",
       "    for item in categoria['itens']:\n",
       "        print(f\"   ‚òê {item}\")\n",
       "    print()\n",
       "\n",
       "print(\"üí° Marque cada item conforme for implementando!\")\n",
       "print(\"üéØ Um deploy bem planejado evita problemas futuros!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **8. Teste R√°pido**\n",
       "\n",
       "Vamos testar seu entendimento sobre deploy:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Teste r√°pido sobre deploy\n",
       "print(\"üß™ TESTE R√ÅPIDO - DEPLOY\\n\")\n",
       "\n",
       "perguntas_teste = [\n",
       "    {\n",
       "        \"pergunta\": \"Qual √© o m√©todo de deploy mais f√°cil para prototipagem?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) API REST\", \n",
       "            \"B) Gradio\", \n",
       "            \"C) Streamlit\"\n",
       "        ],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"Gradio √© o mais f√°cil para prototipagem r√°pida!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"Qual plataforma oferece hospedagem gratuita para aplica√ß√µes de IA?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) AWS\", \n",
       "            \"B) Hugging Face Spaces\", \n",
       "            \"C) Google Cloud\"\n",
       "        ],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"Hugging Face Spaces oferece hospedagem gratuita!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"Qual m√©todo √© melhor para integra√ß√£o com outras aplica√ß√µes?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) Gradio\", \n",
       "            \"B) API REST\", \n",
       "            \"C) Streamlit\"\n",
       "        ],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"API REST permite integra√ß√£o com qualquer aplica√ß√£o!\"\n",
       "    }\n",
       "]\n",
       "\n",
       "for i, q in enumerate(perguntas_teste, 1):\n",
       "    print(f\"‚ùì {i}. {q['pergunta']}\")\n",
       "    for opcao in q['opcoes']:\n",
       "        print(f\"   {opcao}\")\n",
       "    print(f\"üí° Resposta: {q['resposta']} - {q['explicacao']}\")\n",
       "    print()\n",
       "\n",
       "print(\"üéâ Parab√©ns! Voc√™ j√° entende os fundamentos do deploy de modelos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üéâ M√≥dulo 6 Conclu√≠do!**\n",
       "\n",
       "### **O que aprendemos:**\n",
       "\n",
       "‚úÖ **Prepara√ß√£o de modelo** para deploy  \n",
       "‚úÖ **Deploy com Gradio** (interface web simples)  \n",
       "‚úÖ **Deploy com Streamlit** (aplica√ß√£o web robusta)  \n",
       "‚úÖ **Deploy em Hugging Face Spaces** (hospedagem gratuita)  \n",
       "‚úÖ **Deploy com API REST** (integra√ß√£o)  \n",
       "‚úÖ **Compara√ß√£o de m√©todos** de deploy  \n",
       "‚úÖ **Checklist de deploy** completo\n",
       "\n",
       "### **üéì Curso Completo!**\n",
       "\n",
       "üöÄ **M√≥dulo 1**: Introdu√ß√£o ao Fine Tuning  \n",
       "üöÄ **M√≥dulo 2**: Prepara√ß√£o de dados  \n",
       "üöÄ **M√≥dulo 3**: Escolha de modelos  \n",
       "üöÄ **M√≥dulo 4**: Treinamento  \n",
       "üöÄ **M√≥dulo 5**: Avalia√ß√£o  \n",
       "üöÄ **M√≥dulo 6**: Deploy ‚úÖ\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica do Instrutor**: Deploy √© como abrir um restaurante - voc√™ precisa de uma boa localiza√ß√£o (hospedagem), um card√°pio atrativo (interface) e um servi√ßo de qualidade (modelo bem treinado)! Agora sua IA est√° pronta para atender clientes reais! üòÑ\n",
       "\n",
       "**üéâ Parab√©ns! Voc√™ completou o curso de Fine Tuning!**\n",
       "\n",
       "**üöÄ Agora voc√™ √© um treinador de IA!**"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }