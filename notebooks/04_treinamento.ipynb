{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **M√≥dulo 4: Treinando como um personal trainer**\n",
       "\n",
       "## **Aula 4.1: Configura√ß√£o do treinamento**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas como treinar a IA?**\n",
       "\n",
       "Imagine que voc√™ √© um personal trainer e vai treinar um atleta. Voc√™ precisa:\n",
       "- ‚úÖ Definir o objetivo (for√ßa, resist√™ncia, velocidade)\n",
       "- ‚úÖ Criar um plano de treino\n",
       "- ‚úÖ Acompanhar o progresso\n",
       "- ‚úÖ Ajustar quando necess√°rio\n",
       "\n",
       "**Fine Tuning √© a mesma coisa!** Vamos treinar nossa IA para ser especialista no que voc√™ precisa.\n",
       "\n",
       "**Por que o treinamento √© crucial?**\n",
       "\n",
       "√â como ensinar um funcion√°rio:\n",
       "- **Treino ruim**: Funcion√°rio aprende coisas erradas\n",
       "- **Treino bom**: Funcion√°rio vira especialista\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Personal trainer monitorando treino vs configura√ß√£o de treinamento de IA\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Importa√ß√µes necess√°rias\n",
       "import torch\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from transformers import (\n",
       "    AutoTokenizer, AutoModelForCausalLM, \n",
       "    TrainingArguments, Trainer\n",
       ")\n",
       "from peft import LoraConfig, get_peft_model\n",
       "from datasets import Dataset\n",
       "import json\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')\n",
       "\n",
       "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **1. Carregando dados e modelo**\n",
       "\n",
       "Vamos come√ßar carregando nossos ingredientes:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Carregando dataset\n",
       "print(\"üìä CARREGANDO DATASET\\n\")\n",
       "\n",
       "try:\n",
       "    with open('datasets/exemplo_vendas.json', 'r', encoding='utf-8') as f:\n",
       "        dataset_vendas = json.load(f)\n",
       "    \n",
       "    print(f\"‚úÖ Dataset carregado: {len(dataset_vendas)} exemplos\")\n",
       "    \n",
       "    # Mostrando exemplo\n",
       "    print(\"\\nüìù Exemplo do dataset:\")\n",
       "    exemplo = dataset_vendas[0]\n",
       "    print(f\"   Pergunta: {exemplo['messages'][0]['content']}\")\n",
       "    print(f\"   Resposta: {exemplo['messages'][1]['content']}\")\n",
       "    \n",
       "except FileNotFoundError:\n",
       "    print(\"‚ö†Ô∏è  Dataset n√£o encontrado. Criando exemplo...\")\n",
       "    dataset_vendas = [\n",
       "        {\n",
       "            \"messages\": [\n",
       "                {\"role\": \"user\", \"content\": \"Qual √© o melhor iPhone?\"},\n",
       "                {\"role\": \"assistant\", \"content\": \"O iPhone 15 Pro Max √© o melhor para fotografia e performance.\"}\n",
       "            ]\n",
       "        }\n",
       "    ]\n",
       "\n",
       "# Carregando modelo pequeno para demonstra√ß√£o\n",
       "print(\"\\nü§ñ CARREGANDO MODELO\\n\")\n",
       "\n",
       "model_name = \"microsoft/DialoGPT-small\"\n",
       "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "\n",
       "print(f\"‚úÖ Modelo carregado: {model_name}\")\n",
       "print(f\"üìä Par√¢metros: {model.num_parameters():,}\")\n",
       "\n",
       "# Configurando tokenizer\n",
       "if tokenizer.pad_token is None:\n",
       "    tokenizer.pad_token = tokenizer.eos_token\n",
       "\n",
       "print(\"‚úÖ Tokenizer configurado!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **2. Configurando LoRA**\n",
       "\n",
       "Vamos configurar o LoRA para treinamento eficiente:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configurando LoRA\n",
       "print(\"üîß CONFIGURANDO LORA\\n\")\n",
       "\n",
       "# Configura√ß√£o LoRA\n",
       "lora_config = LoraConfig(\n",
       "    r=16,  # Rank da decomposi√ß√£o\n",
       "    lora_alpha=32,  # Escala dos pesos LoRA\n",
       "    target_modules=[\"q_proj\", \"v_proj\"],  # M√≥dulos para aplicar LoRA\n",
       "    lora_dropout=0.1,  # Dropout para regulariza√ß√£o\n",
       "    bias=\"none\",  # Como tratar bias\n",
       "    task_type=\"CAUSAL_LM\"  # Tipo de tarefa\n",
       ")\n",
       "\n",
       "print(\"üìã Configura√ß√£o LoRA:\")\n",
       "print(f\"   Rank (r): {lora_config.r}\")\n",
       "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
       "print(f\"   Target modules: {lora_config.target_modules}\")\n",
       "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
       "\n",
       "# Aplicando LoRA ao modelo\n",
       "model = get_peft_model(model, lora_config)\n",
       "\n",
       "print(f\"\\n‚úÖ LoRA aplicado!\")\n",
       "print(f\"üìä Par√¢metros trein√°veis: {model.print_trainable_parameters()}\")\n",
       "\n",
       "print(\"\\nüí° LoRA permite treinar apenas uma pequena parte do modelo, economizando mem√≥ria e tempo!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **3. Preparando dados para treinamento**\n",
       "\n",
       "Vamos preparar os dados no formato correto:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Fun√ß√£o para preparar dados\n",
       "def preparar_dados_treinamento(dataset, tokenizer, max_length=512):\n",
       "    \"\"\"\n",
       "    Prepara dados para treinamento\n",
       "    \"\"\"\n",
       "    dados_processados = []\n",
       "    \n",
       "    for exemplo in dataset:\n",
       "        # Formatando como conversa\n",
       "        texto = f\"User: {exemplo['messages'][0]['content']}\\nAssistant: {exemplo['messages'][1]['content']}\"\n",
       "        \n",
       "        # Tokenizando\n",
       "        tokens = tokenizer(\n",
       "            texto,\n",
       "            truncation=True,\n",
       "            max_length=max_length,\n",
       "            padding=\"max_length\",\n",
       "            return_tensors=\"pt\"\n",
       "        )\n",
       "        \n",
       "        dados_processados.append({\n",
       "            \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
       "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(),\n",
       "            \"labels\": tokens[\"input_ids\"].squeeze().clone()\n",
       "        })\n",
       "    \n",
       "    return dados_processados\n",
       "\n",
       "# Preparando dados\n",
       "print(\"üîÑ PREPARANDO DADOS PARA TREINAMENTO\\n\")\n",
       "\n",
       "dados_treinamento = preparar_dados_treinamento(dataset_vendas, tokenizer)\n",
       "\n",
       "print(f\"‚úÖ Dados preparados: {len(dados_treinamento)} exemplos\")\n",
       "\n",
       "# Criando dataset do Hugging Face\n",
       "dataset_hf = Dataset.from_list(dados_treinamento)\n",
       "\n",
       "print(f\"üìä Dataset criado: {len(dataset_hf)} exemplos\")\n",
       "print(f\"üîß Colunas: {dataset_hf.column_names}\")\n",
       "\n",
       "# Mostrando exemplo processado\n",
       "print(\"\\nüìù Exemplo processado:\")\n",
       "exemplo_processado = dataset_hf[0]\n",
       "print(f\"   Input IDs shape: {exemplo_processado['input_ids'].shape}\")\n",
       "print(f\"   Attention mask shape: {exemplo_processado['attention_mask'].shape}\")\n",
       "print(f\"   Labels shape: {exemplo_processado['labels'].shape}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **4. Configurando argumentos de treinamento**\n",
       "\n",
       "Agora vamos configurar os par√¢metros do treinamento:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configurando argumentos de treinamento\n",
       "print(\"‚öôÔ∏è CONFIGURANDO ARGUMENTOS DE TREINAMENTO\\n\")\n",
       "\n",
       "training_args = TrainingArguments(\n",
       "    output_dir=\"./checkpoints\",  # Diret√≥rio para salvar checkpoints\n",
       "    num_train_epochs=3,  # N√∫mero de √©pocas\n",
       "    per_device_train_batch_size=2,  # Batch size por dispositivo\n",
       "    gradient_accumulation_steps=4,  # Acumula√ß√£o de gradientes\n",
       "    learning_rate=2e-4,  # Taxa de aprendizado\n",
       "    warmup_steps=100,  # Passos de aquecimento\n",
       "    logging_steps=10,  # Frequ√™ncia de logs\n",
       "    save_steps=500,  # Frequ√™ncia de salvamento\n",
       "    evaluation_strategy=\"no\",  # Sem avalia√ß√£o durante treino\n",
       "    save_total_limit=2,  # M√°ximo de checkpoints salvos\n",
       "    remove_unused_columns=False,  # Manter colunas n√£o usadas\n",
       "    dataloader_pin_memory=False,  # Otimiza√ß√£o de mem√≥ria\n",
       "    report_to=None,  # Sem relat√≥rios externos\n",
       ")\n",
       "\n",
       "print(\"üìã Configura√ß√£o de treinamento:\")\n",
       "print(f\"   √âpocas: {training_args.num_train_epochs}\")\n",
       "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
       "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
       "print(f\"   Warmup steps: {training_args.warmup_steps}\")\n",
       "print(f\"   Output dir: {training_args.output_dir}\")\n",
       "\n",
       "print(\"\\nüí° Estes par√¢metros s√£o otimizados para treinamento r√°pido e eficiente!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **5. Iniciando o treinamento**\n",
       "\n",
       "Agora vamos treinar nosso modelo:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Iniciando treinamento\n",
       "print(\"üöÄ INICIANDO TREINAMENTO\\n\")\n",
       "\n",
       "# Criando trainer\n",
       "trainer = Trainer(\n",
       "    model=model,\n",
       "    args=training_args,\n",
       "    train_dataset=dataset_hf,\n",
       "    tokenizer=tokenizer,\n",
       ")\n",
       "\n",
       "print(\"‚úÖ Trainer criado!\")\n",
       "print(\"\\nüéØ Iniciando treinamento...\")\n",
       "print(\"‚è±Ô∏è  Isso pode levar alguns minutos...\")\n",
       "\n",
       "# Treinamento (comentado para demonstra√ß√£o)\n",
       "try:\n",
       "    # Descomente a linha abaixo para treinar de verdade\n",
       "    # trainer.train()\n",
       "    print(\"\\n‚ö†Ô∏è  Treinamento comentado para demonstra√ß√£o\")\n",
       "    print(\"üí° Para treinar de verdade, descomente a linha trainer.train()\")\n",
       "    \n",
       "except Exception as e:\n",
       "    print(f\"\\n‚ùå Erro no treinamento: {e}\")\n",
       "\n",
       "print(\"\\n‚úÖ Configura√ß√£o de treinamento conclu√≠da!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **6. Monitoramento e m√©tricas**\n",
       "\n",
       "Vamos criar fun√ß√µes para monitorar o treinamento:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Fun√ß√µes de monitoramento\n",
       "def plot_training_metrics(losses, learning_rates):\n",
       "    \"\"\"\n",
       "    Plota m√©tricas de treinamento\n",
       "    \"\"\"\n",
       "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
       "    \n",
       "    # Plot de loss\n",
       "    ax1.plot(losses, 'b-', linewidth=2)\n",
       "    ax1.set_title('üìâ Loss de Treinamento')\n",
       "    ax1.set_xlabel('Steps')\n",
       "    ax1.set_ylabel('Loss')\n",
       "    ax1.grid(True, alpha=0.3)\n",
       "    \n",
       "    # Plot de learning rate\n",
       "    ax2.plot(learning_rates, 'r-', linewidth=2)\n",
       "    ax2.set_title('üìà Learning Rate')\n",
       "    ax2.set_xlabel('Steps')\n",
       "    ax2.set_ylabel('Learning Rate')\n",
       "    ax2.grid(True, alpha=0.3)\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "\n",
       "# Simulando m√©tricas de treinamento\n",
       "print(\"üìä SIMULANDO M√âTRICAS DE TREINAMENTO\\n\")\n",
       "\n",
       "steps = list(range(0, 100, 10))\n",
       "losses = [2.5, 2.1, 1.8, 1.5, 1.3, 1.1, 0.9, 0.8, 0.7, 0.6]  # Loss diminuindo\n",
       "learning_rates = [0.0002, 0.00018, 0.00016, 0.00014, 0.00012, 0.0001, 0.00008, 0.00006, 0.00004, 0.00002]  # LR diminuindo\n",
       "\n",
       "print(\"üìà M√©tricas simuladas:\")\n",
       "print(f\"   Loss inicial: {losses[0]:.2f}\")\n",
       "print(f\"   Loss final: {losses[-1]:.2f}\")\n",
       "print(f\"   Melhoria: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")\n",
       "\n",
       "# Plotando m√©tricas\n",
       "plot_training_metrics(losses, learning_rates)\n",
       "\n",
       "print(\"üí° Loss diminuindo = modelo aprendendo! Learning rate diminuindo = converg√™ncia!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **7. Salvando o modelo treinado**\n",
       "\n",
       "Vamos salvar nosso modelo treinado:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Salvando modelo treinado\n",
       "print(\"üíæ SALVANDO MODELO TREINADO\\n\")\n",
       "\n",
       "def salvar_modelo(model, tokenizer, output_dir):\n",
       "    \"\"\"\n",
       "    Salva o modelo treinado\n",
       "    \"\"\"\n",
       "    try:\n",
       "        # Salvando modelo LoRA\n",
       "        model.save_pretrained(output_dir)\n",
       "        \n",
       "        # Salvando tokenizer\n",
       "        tokenizer.save_pretrained(output_dir)\n",
       "        \n",
       "        print(f\"‚úÖ Modelo salvo em: {output_dir}\")\n",
       "        \n",
       "        # Salvando configura√ß√£o\n",
       "        config_info = {\n",
       "            \"model_name\": \"microsoft/DialoGPT-small\",\n",
       "            \"training_epochs\": 3,\n",
       "            \"learning_rate\": 2e-4,\n",
       "            \"dataset_size\": len(dataset_vendas),\n",
       "            \"lora_config\": {\n",
       "                \"r\": 16,\n",
       "                \"alpha\": 32,\n",
       "                \"dropout\": 0.1\n",
       "            }\n",
       "        }\n",
       "        \n",
       "        with open(f\"{output_dir}/config.json\", 'w') as f:\n",
       "            json.dump(config_info, f, indent=2)\n",
       "        \n",
       "        print(f\"‚úÖ Configura√ß√£o salva em: {output_dir}/config.json\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro ao salvar: {e}\")\n",
       "\n",
       "# Salvando modelo\n",
       "output_dir = \"./models/iphone_assistant\"\n",
       "salvar_modelo(model, tokenizer, output_dir)\n",
       "\n",
       "print(\"\\nüéâ Modelo treinado e salvo com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **8. Teste R√°pido**\n",
       "\n",
       "Vamos testar seu entendimento sobre treinamento:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Teste r√°pido sobre treinamento\n",
       "print(\"üß™ TESTE R√ÅPIDO - TREINAMENTO\\n\")\n",
       "\n",
       "perguntas_teste = [\n",
       "    {\n",
       "        \"pergunta\": \"Qual √© o objetivo do LoRA no fine tuning?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) Aumentar a velocidade\", \n",
       "            \"B) Reduzir o uso de mem√≥ria\", \n",
       "            \"C) Melhorar a qualidade\"\n",
       "        ],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"LoRA reduz o uso de mem√≥ria treinando apenas uma pequena parte do modelo!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"O que significa quando a loss diminui durante o treinamento?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) O modelo est√° piorando\", \n",
       "            \"B) O modelo est√° aprendendo\", \n",
       "            \"C) O modelo est√° quebrando\"\n",
       "        ],\n",
       "        \"resposta\": \"B\",\n",
       "        \"explicacao\": \"Loss diminuindo significa que o modelo est√° aprendendo e melhorando!\"\n",
       "    },\n",
       "    {\n",
       "        \"pergunta\": \"Quantas √©pocas s√£o recomendadas para fine tuning?\",\n",
       "        \"opcoes\": [\n",
       "            \"A) 1-5 √©pocas\", \n",
       "            \"B) 10-20 √©pocas\", \n",
       "            \"C) 50+ √©pocas\"\n",
       "        ],\n",
       "        \"resposta\": \"A\",\n",
       "        \"explicacao\": \"1-5 √©pocas s√£o suficientes para fine tuning, evitando overfitting!\"\n",
       "    }\n",
       "]\n",
       "\n",
       "for i, q in enumerate(perguntas_teste, 1):\n",
       "    print(f\"‚ùì {i}. {q['pergunta']}\")\n",
       "    for opcao in q['opcoes']:\n",
       "        print(f\"   {opcao}\")\n",
       "    print(f\"üí° Resposta: {q['resposta']} - {q['explicacao']}\")\n",
       "    print()\n",
       "\n",
       "print(\"üéâ Parab√©ns! Voc√™ j√° entende os fundamentos do treinamento!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üéâ M√≥dulo 4 Conclu√≠do!**\n",
       "\n",
       "### **O que aprendemos:**\n",
       "\n",
       "‚úÖ **Configura√ß√£o de LoRA** para treinamento eficiente  \n",
       "‚úÖ **Prepara√ß√£o de dados** para treinamento  \n",
       "‚úÖ **Configura√ß√£o de argumentos** de treinamento  \n",
       "‚úÖ **Inicializa√ß√£o do treinamento**  \n",
       "‚úÖ **Monitoramento de m√©tricas**  \n",
       "‚úÖ **Salvamento do modelo** treinado\n",
       "\n",
       "### **Pr√≥ximos Passos:**\n",
       "\n",
       "üöÄ **M√≥dulo 5**: Avaliando como um professor corrige prova  \n",
       "üöÄ **M√≥dulo 6**: Deploy como abrir um restaurante\n",
       "\n",
       "---\n",
       "\n",
       "**üí° Dica do Instrutor**: Treinar uma IA √© como treinar um atleta - precisa de paci√™ncia, monitoramento e ajustes! Agora que treinamos nossa IA, vamos ver se ela aprendeu bem! üòÑ\n",
       "\n",
       "**üöÄ Pr√≥ximo m√≥dulo**: Vamos avaliar a qualidade do nosso modelo treinado!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }